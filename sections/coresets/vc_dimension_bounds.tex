\subsubsection{Bounding the VC-dimension}

Recall, that in the core theorem of the sensitivity
framework (see Theorem~\ref{theorem:sensitivity-framework}),
the function-set that induces our range space of interest
is defined as
\begin{equation*}
    F^\ast = \left\{ \frac{S w_i}{s_i |R|} g_i(\beta) \ |\ i \in [n] \right\},
\end{equation*}
where $s_i$ are upper bounds on the sensitivities, $S$ is the
sum of these upper bounds, $|R|$ is the size of the sample and
$w_i$ are the initial weights.
We are thus dealing with a set of weighted probit loss functions.

In order to approach the complex problem of bounding the VC-dimension
of a set of arbitrarily weighted probit loss functions, we first
deal with the slightly simpler problem that arises when the weights
only consist of a single positive constant, i.e. we are looking at the
set $\mathcal{F}^c = \{ c g_i(\beta) \ | \ i \in [n] \}$.
The authors of \cite{scalable-bayesian-logreg} showed, that
in the case of the logistic loss,
it is possible to relate the VC-dimension of the
range space induced by $\mathcal{F}^c$ to the VC-dimension of
the affine hyperplane classifier, which the authors of
\cite{computational-learning-theory} showed to be bounded by $d + 1$.
It turns out, that a similar case can be made for the probit
loss, which we demonstrate in the following lemma.

\begin{lemma}[cf. \cite{scalable-bayesian-logreg}]
    \label{lemma:vcdim-constant}
    Let $Z \in \mathbb{R}^{n \times d}$, let $z_i \in \mathbb{R}^d$ be the
    $i$-th row of $Z$ and let $c \in \mathbb{R}_{>0}$.
    Let $F = \{g_1, ..., g_n\}$ be a set of functions with
    $g_i(\beta) = g(z_i^T \beta)$, where
    $g(x) = \ln\left(\frac{1}{1 - \Phi(x)}\right)$ is the
    probit loss.
    The VC-dimension of the range space induced by
    \begin{equation*}
        \mathcal{F}^c = \{ c g_i(\beta) \ | \ i \in [n] \}
    \end{equation*}
    is bounded by $\Delta(\mathfrak{R}_{\mathcal{F}^c}) \leq d + 1$.
\end{lemma}
\begin{proof}
    We start by noting that for all $G \subseteq \mathcal{F}^c$ we have
    \begin{equation*}
        \left| \left\{ G \cap R \ | \
        R \in \textup{ranges}(\mathcal{F}^c) \right\} \right|
        =
        \left| \left\{ \textup{range}(G, \beta, r) \ | \
        \beta \in \mathbb{R}^d, \ r \geq 0 \right\} \right|.
    \end{equation*}
    Since $g$ is invertible and monotonous, we have for all
    $\beta \in \mathbb{R}^d$ and $r \geq 0$, that
    \begin{align*}
        \textup{range}(G, \beta, r)
         & = \left\{ g_i \in G \ |\ c g_i(\beta) \geq r \right\}                                 \\
         & = \left\{ g_i \in G \ |\ c g(z_i^T \beta) \geq r \right\}                             \\
         & = \left\{ g_i \in G \ |\ z_i^T \beta \geq g^{-1} \left( \frac{r}{c} \right) \right\}.
    \end{align*}
    Note, that $\left\{ g_i \in G
        \ |\ z_i^T \beta \geq g^{-1} \left( \frac{r}{c} \right) \right\}$
    corresponds to the positively classified points of the
    affine hyperplane classifier
    $x \mapsto \textup{sign}\left(
        x^T\beta - g^{-1}\left(\frac{r}{c}\right) \right)$.
    Due to the fact, that $g^{-1}$ is a surjective function,
    we thus have for all $G \subseteq \mathcal{F}^c$, that
    \begin{equation*}
        \left| \left\{ G \cap R \ | \
        R \in \textup{ranges}(\mathcal{F}^c) \right\} \right|
        =
        \left| \left\{ \left\{ g_i \in G
        \ |\ z_i^T \beta - s \geq 0 \right\} \ | \
        \beta \in \mathbb{R}^d, \ s \in \mathbb{R} \right\} \right|.
    \end{equation*}
    As shown in \cite{computational-learning-theory}, the VC-dimension
    of the set of affine hyperplane classifiers is
    $d+1$, so it follows that
    $\Delta(\mathfrak{R}_{\mathcal{F}^c}) \leq d + 1$,
    which concludes the proof.
\end{proof}

In the next step, we will generalize the class $\mathcal{F}^c$
of constantly weighted probit loss functions to the class
$\mathcal{F}^w = \{w_ig_i(\beta)\ |\ i \in [n]\}$ of arbitrarily
weighted probit loss functions for a weight vector
$w \in \mathbb{R}^n_{>0}$, which also includes $F^\ast$,
our class of interest.
The authors of \cite{on-coresets} presented a proof, that shows
how the VC-dimension of the range space induced by such a
set can be bounded by $t\cdot(d+1)$ in the case of logistic
regression, where $t \in \mathbb{N}$ is the number of distinct
weights in the vector $w$.
We follow a similar path and adapt their argument to the
context of probit regression in the following lemma.

\begin{lemma}[cf. \cite{on-coresets}]
    \label{lemma:vcdim-arbitrary}
    Let $Z \in \mathbb{R}^{n \times d}$, let $z_i \in \mathbb{R}^d$ be the
    $i$-th row of $Z$ and let
    $w \in \mathbb{R}^n_{>0}$ be a vector of positive weights,
    where $w_i \in \{ v_1, ..., v_t \}$ for all $i \in [n]$.
    Let $F = \{g_1, ..., g_n\}$ be a set of functions with
    $g_i(\beta) = g(z_i^T \beta)$.
    The VC-dimension of the range space induced by
    \begin{equation*}
        \mathcal{F}^w = \left\{ w_ig_i(\beta) \ |\ i \in [n] \right\}
    \end{equation*}
    is bounded by
    $\Delta(\mathfrak{R}_{\mathcal{F}^w}) \leq t \cdot (d + 1)$.
\end{lemma}
\begin{proof}
    We start by partitioning the functions in
    $\mathcal{F}^w$ into $t$ disjoint classes
    \begin{equation*}
        F_j = \{ w_ig(z_i\beta) \in \mathcal{F}^w \
        |\ w_i = v_j \},\quad j \in [t].
    \end{equation*}
    The functions in each of these classes have an equal
    weight, wich means that by Lemma~\ref{lemma:vcdim-constant}, each of
    their induced range spaces has a VC-dimension of at most $d+1$.

    For the sake of contradiction, assume that
    $\Delta(\mathfrak{R}_{\mathcal{F}^w}) > t \cdot (d + 1)$ and let
    $G$ be the corresponding set of size $|G| > t \cdot (d + 1)$ that
    is shattered by $\text{ranges}(\mathcal{F}^w)$.
    Since the sets $F_j$ are disjoint, each intersection
    $F_j \cap G$ must be shattered by $\text{ranges}(F_j)$ as well.
    Further, at least one of the intersections must have at minimum
    $\frac{|G|}{t}$ elements, which means that for at least one $j \in [t]$
    it holds that
    $|F_j \cap G| \geq \frac{|G|}{t} > \frac{t \cdot (d+1)}{t} = d + 1$.
    This is a contradiction to Lemma~\ref{lemma:vcdim-constant}, which
    concludes the proof.
\end{proof}

We have now found a way to bound the VC-dimension of the range space
induced by $F^\ast$ in the number of distinct weights, i.e.
the number of distinct values of $\frac{S w_i}{s_i |R|}$.
But there is one important issue that remains to be dealt with:
In the general case, we don't know the number of distinct
values of $\frac{S w_i}{s_i |R|}$, and it is even reasonable to
assume, that this value can be equal
to the total number of datapoints, $n$.
This would be a problem for us though, because the core theorem
of the sensitivity framework
(Theorem \ref{theorem:sensitivity-framework}) tells us,
that the size of our
coreset will depend linearly on the VC-dimension of the range
space induced by $F^\ast$. If this VC-dimension is in $\Omega(n)$,
our coresets won't be small. It follows, that we have to find
a way to work around that problem in order to obtain small coresets.
