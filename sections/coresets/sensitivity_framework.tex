\subsection{The Sensitivity Framework}

After having imposed some reasonable restrictions on the
datasets under study, it is now time to think about
how an algorithm that selects a coreset
$\mathcal{C} \subseteq \mathcal{D}$ could be constructed.
One of the first ideas that come to mind to solve such a problem
is the process of random sampling.
After all, why don't we just randomly select a subset of
points from $\mathcal{D}$ of the desired size?
Wouldn't that already solve our problem?

The issue with this approach is that it cannot be guaranteed
that such a uniform random sample will yield a coreset,
i.e. a subset of $\mathcal{D}$ that helps us to obtain a
$(1 \pm \epsilon)$-approximation of the original loss function.
As we will later see in the experiments section, uniform sampling
works reasonably well when the data is well behaved, but fails
terribly when there are a few very important datapoints that
it tends to miss.
Intuitively speaking,
if there are lots of points in a dataset that don't influence the
loss function much, but only a few points that have a big impact
on the loss function, uniform sampling fails because it tends
to miss the few very important points.

It turns out, that one way to remedy the downsides of uniform sampling
is to include a measure of importance in the sampling distribution.
Instead of sampling each datapoint with equal probability,
why don't we construct our sampling distribution in such a way,
that the impact of each point on the loss function is taken into
account? This way, more important points would be assigned
a higher probability to be sampled and less important points
would conversely be assigned a lower sampling probability.
This idea forms the basis of the so called
\textit{sensitivity framework}, an algorithmic framework
introduced in~\cite{feldman-langberg-coresets}
(see also \cite{big-data-tiny-data}), that aims to
find coresets by randomly sampling points proportional to
their importance for the loss function.

In the sensitivity framework, the importance of a point
in a dataset can be thought of the maximum proportion of
the loss function that it can take up in the worst case.
To formalize this intuition, the sensitivity framework shifts
the representation of a dataset as a collection of
points towards the representation as a collection of
\textit{functions}, where each function represents the
loss of a point.
To explain what that means, consider the dataset
$\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$ with scaled model
matrix $Z$, i.e. the rows of $Z$ are given by vectors
$z_i = -(2y_i-1)x_i$ and the loss function
$f(\beta) = \sum_{i=1}^n w_ig(z_i^T\beta)$, where
$w_1, ..., w_n$ are positive weights.
From now on, we will assign to each point in the dataset the
function $g_i(\beta) = g(z_i^T\beta)$, that represents its
individual contribution to the overall loss function.
This way, we can equivalently represent the dataset $\mathcal{D}$
as the set of functions $F = \{g_1, ..., g_n\}$.

Having made this conceptual change of representing a dataset as
a set of functions, we can now use this new representation to
formalize the concept of importance for each point, according
to wich we later want to sample.
As already hinted at, the importance of a point will be the
maximum share of the loss function, that the loss of the specific
point will take up in the worst case.
This worst case importance is also called the \textit{sensitivity}
of a point, and it was first introduced in
\cite{langberg-schulman-sensitivities}.
A formal definition of this concept, which forms
the basis of the sensitivity framework, is given below.

\begin{definition}[\cite{langberg-schulman-sensitivities}]
    \label{def:sensitivity}
    Let $F = \{ g_1, ..., g_n \}$ be a set of functions,
    $g_i: \mathbb{R}^d \rightarrow \mathbb{R}_{\geq 0}, \ i \in [n]$
    and let $w \in \mathbb{R}^n_{>0}$ be a vector of positive weights.
    The sensitivity of $g_i$ for $f_w(\beta) = \sum_{i=1}^n w_i g_i(\beta)$ is defined as
    \begin{equation*}
        \varsigma_i = \sup_{\beta \in \mathbb{R}^d, \ f_w(\beta) > 0} \frac{w_i g_i(\beta)}{f_w(\beta)}.
    \end{equation*}
    The total sensitivity, i.e. the sum of the sensitivities is $\mathfrak{S} = \sum_{i=1}^n \varsigma_i$.
\end{definition}

The idea behind the sensitivity framework is to sample from a
distribution where the sampling probabilities are determined by
the sensitivities, i.e. the worst case importance of each point
in the dataset.
But there is one issue remaining: The true sensitivities
$\varsigma_1, ..., \varsigma_n$ are unknown, and as
pointed out in~\cite{braverman-feldman-coresets}, their
computation requires solving the original optimization problem,
which we wanted to avoid in the first place.
Luckily, there exists a simple workaround:
Instead of sampling proportionally to the true sensitivities,
we can instead also sample proportionally to
upper bounds $s_i$, where $s_i \geq \varsigma_i$, that are
potentially easier to compute.

There is one caveat though, that we have to take into account:
We will later see, that in the sensitivity framework, the
size of the coreset that we obtain through sampling proportionally
to upper bounds of the sensitivities, is directly influenced
by the sum of said bounds: $S = \sum_{i=1}^n s_i$.
It follows, that we have to find bounds that are as tight as
possible, so that our coreset won't be unnecessarily large,
which will be one of the main contributions of this work.

The way in wich the authors of~\cite{feldman-langberg-coresets}
were able to show that sampling proportionally to upper bounds of the
sensitivities can lead to provably small coresets,
was to relate the concept of sensitivities to
the theory of so-called \textit{range spaces} and
the \textit{VC-dimension}
(see for example \cite{computational-learning-theory} for an
introduction of the VC-dimension).
We introduce these concepts in the following definitions, because
they will also turn out to be of crucial importance to the
size of our coresets.

\begin{definition}[\cite{feldman-langberg-coresets}]
    \label{def:range-space}
    A range space is a pair $\mathfrak{R} = (F, \textup{ranges})$, where F is a set
    and $\textup{ranges}$ is a family (set) of subsets of F.
\end{definition}

\begin{definition}[\cite{feldman-langberg-coresets}]
    \label{def:vc-dimension}
    The VC-dimension $\Delta(\mathfrak{R})$ of a range space
    $\mathfrak{R} = (F, \textup{ranges})$ is
    the size $|G|$ of the largest subset $G \subseteq F$ such that
    \begin{equation*}
        \left| \left\{ G \cap R \ | \ R \in \textup{ranges} \right\} \right|
        = 2^{|G|},
    \end{equation*}
    i.e. $G$ is shattered by $\textup{ranges}$.
\end{definition}

The most important part to understand about the two preceding definitions
is the concept of \textit{shattering}.
Given a set $F$, a set of subsets of $F$ called ranges and a subset
$G \subseteq F$, what does it mean if $G$ is shattered by ranges?
Definition~\ref{def:vc-dimension} says, that the set of intersections
between $G$ and ranges must be equal to $2^{|G|}$, which is exactly
the size of the set of all subsets of $G$.
It follows, that for $G$ to be shattered by ranges, every subset
of $G$ must apper in (be a subset of) at least one of the sets in ranges.
From this it also immediately follows, that if $G$ is shattered by
ranges, then every subset of $G$ is also shattered by ranges.
Thus, the VC-dimension of the range space that is given by $F$ and
ranges is simply the size of the largest subset $G \subseteq F$,
such that every subset of
$G$ appears in at least one element of ranges.

Instead of dealing with arbitrary sets like in
definition~\ref{def:range-space} and definition~\ref{def:vc-dimension},
we are specifically dealing with a set of functions
$F = \{g_1, ..., g_n\}$, that represents our dataset.
In the next definition, we will see, how such a set of functions
can be used to \textit{induce} a range space, which will
be of crucial importance when limiting the size of
our coresets later on.

\begin{definition}[\cite{feldman-langberg-coresets}]
    \label{def:induced-range-space}
    Let $F$ be a finite set of functions mapping from $\mathbb{R}^d$ to
    $\mathbb{R}_{\geq 0}$.
    For every $\beta \in \mathbb{R}^d$ and $r \geq 0$, let
    \begin{equation*}
        \textup{range}(F, \beta, r) = \left\{ f \in F \ | \  f(\beta) \geq r  \right\}
    \end{equation*}
    and let
    \begin{equation*}
        \textup{ranges}(F) = \left\{ \textup{range}(F, \beta, r) \ | \ \beta \in \mathbb{R}^d, \ r \geq 0  \right\}.
    \end{equation*}
    Then we call $\mathfrak{R}_F := (F, \textup{ranges}(F))$ the range space induced by F.
\end{definition}

To understand what it means for a set of functions $F$ to induce
a range space, consider that we can always partition the set
of functions into two disjoint subsets by choosing
a specific $\beta$ and applying a threshold $r$:
Every element in $F$ with a value of $f(\beta) \geq r$ goes to
one subset, the remainder goes to the other subset.
Considering that our functions actually represent datapoints,
each value of $\beta$ and $r$ give one way to partition the
datapoints into two disjoint subsets.
Thus, the set $\text{ranges}(F)$ in definition~\ref{def:induced-range-space}
can be thought of the set of all possible partitions of
functions (datapoints), that can be obtained for all
possible $\beta \in \mathbb{R}^d$ and $r \geq 0$ and the
VC-dimension of the induced range space is the size of the
biggest set of datapoints, that can be arbitrarily partitioned
by different values of $\beta$ and $r$.

We are now ready for the full theorem that relates sensitivity sampling
to the theory of range spaces and the VC-dimension, which forms
the core of the sensitivity framework.
Its original version goes back to~\cite{feldman-langberg-coresets}
and it was further improved in~\cite{braverman-feldman-coresets}.
The version we are presenting here is a slightly
adapted variant introduced by~\cite{big-data-tiny-data}:

\begin{theorem}[\cite{big-data-tiny-data}]
    \label{theorem:sensitivity-framework}
    Let $F = \{ g_1, ..., g_n \}$ be a finite set of functions
    mapping from $\mathbb{R}^d$ to $\mathbb{R}_{\geq 0}$.
    Let $w \in \mathbb{R}^n_{>0}$ be a vector of positive weights.
    Let $\epsilon, \delta \in (0, \frac{1}{2})$.
    Let $s_i \geq \varsigma_i$ be upper bounds of the sensitivities and
    let $S = \sum_{i=1}^n s_i$.
    Given $s_i$, one can compute in time $O(|F|)$ a set
    $R \subseteq F$ of
    \begin{equation*}
        |R| \in O \left( \frac{S}{\epsilon^2} \left( \Delta \log S + \log \left( \frac{1}{\delta} \right) \right) \right)
    \end{equation*}
    weighted functions, such that with probability $1 - \delta$ we have
    for all $\beta \in \mathbb{R}^d$ simultaneously
    \begin{equation*}
        (1-\epsilon) \sum_{g_i \in F} w_i g_i(\beta) \leq \sum_{g_i \in R} u_i g_i(\beta) \leq (1 + \epsilon) \sum_{g_i \in F} w_i g_i(\beta).
    \end{equation*}
    Each element of $R$ is sampled independently with probability
    $p_j = \frac{s_j}{S}$ from $F$, $u_i = \frac{S w_j}{s_j |R|}$
    denotes the weight of a function $g_i \in R$ that corresponds to
    $g_j \in F$ and $\Delta$ is an upper bound on the
    VC-dimension of the range space $\mathfrak{R}_{F^\ast}$ induced by
    $F^\ast$, where $F^\ast$ is the set of functions $g_i \in F$
    scaled by $\frac{S w_i}{s_i |R|}$, i.e.
    $F^\ast = \left\{ \frac{S w_i}{s_i |R|} g_i(\beta) \ |\ i \in [n] \right\}$.
\end{theorem}
