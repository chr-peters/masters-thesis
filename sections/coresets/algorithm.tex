\subsubsection{A First Na\"ive Algorithm}
\label{sec:naive-algorihtm}

It now remains to solve the final challenge: How can we limit the
number of distinct weights in $F^\ast$ in order to limit
the VC-dimension of the range space induced by $F^\ast$ and obtain
small coresets? The authors of \cite{on-coresets} were facing
a similar situation, and it turns out that they managed to come
up with a very clever idea: Their approach to the problem is to
slightly increase the upper bounds on the sensitivities $s_i$ to
a new value $s_i' \geq s_i$, such that the fraction $\frac{s_i'}{w_i}$ is
exactly a power of two. This way, the total sum of the sensitivities
$S' = \sum_{i=1}^n s_i'$ is still under control, because as a worst
case bound we have that $S' \leq 2 S$, which doesn't influence the
big-O notation and we still have $S' \in O(\mu d)$.
The big advantage of this approach is, that we now have a way
to bound the number of distinct values of $\frac{S'w_i}{s_i' |R|}$
in a term that is logarithmic in $n$.
To see why this is the case, we first derive two simple
inequalities. The first one goes like this and holds for all
$i \in [n]$:
\begin{equation*}
    \frac{s_i'}{w_i} \leq \frac{2 s_i}{w_i}
    = \frac{2 (80 + 16\mu)(\lVert U_i \rVert_2^2 + \frac{w_i}{\mathcal{W}})}{w_i}
    \leq \frac{192\mu (\lVert U_i \rVert_2^2 + \frac{w_i}{\mathcal{W}})}{w_i}
    \leq \frac{384 \mu}{w_i}
    \leq \frac{384 \mu}{w_{min}},
\end{equation*}
where $w_{min}$ is the minimum weight and $\mathcal{W}$ is the sum
of all weights. The third inequality holds, because it is
always true that $\mu \geq 1$ and the fourth inequality is true because
one property of the statistical leverage scores
is that $\lVert U_i \rVert_2^2 \leq 1$.

Next, we show how $\frac{s_i'}{w_i}$ can be lower-bounded for all
$i \in [n]$:
\begin{equation*}
    \frac{s_i'}{w_i} \geq \frac{s_i}{w_i}
    \geq \sup_{\beta \in \mathbb{R}^d} \frac{g_i(\beta)}{\sum_{i=1}^n w_ig_i(\beta)}
    \overset{\beta = 0}{\geq} \frac{1}{\sum_{i=1}^n w_i}
    \geq \frac{1}{n w_{max}},
\end{equation*}
where $w_{max}$ is the maximum weight. Putting both of the inequalities
together, we thus have that
\begin{equation*}
    \frac{1}{n w_{max}} \leq \frac{s_i'}{w_i} \leq \frac{384\mu}{w_{min}}.
\end{equation*}
We know, that the values of $\frac{s_i'}{w_i}$ are exactly powers
of two, so we can compute the amount of possible distinct values of
$\frac{s_i'}{w_i}$, which we call $t$, like this:
\begin{equation*}
    t \leq \log_2\left( \frac{384 \mu}{w_{min}}\right)
    - \log_2\left( \frac{1}{n w_{max}}\right)
    = \log_2\left(384\mu n \frac{w_{max}}{w_{min}}\right)
    \in O\left(\log_2(\mu n \omega)\right),
\end{equation*}
where $\omega = \frac{w_{max}}{w_{min}}$.
Thus, when sampling according to $s_i'$, our function class of interest
becomes
\begin{equation*}
    F^{\ast'} = \left\{ \frac{S' w_i}{s_i' |R|} g_i(\beta) \ |\ i \in [n] \right\},
\end{equation*}
and the weights $\frac{S' w_i}{s_i' |R|}$ can assume only
$O(\log_2(\mu n \omega))$ distinct values.
Plugging this into Lemma~\ref{lemma:vcdim-arbitrary}, we get that
the VC-dimension of the range space induced by $F^{\ast'}$ is
upper bounded by a term in $O(d \log_2(\mu n \omega))$.

As a side note, one might ask if the term
$\omega = \frac{w_{max}}{w_{min}}$ could be a problem in practical
applications. To answer this concern, we remark that generally in most
situations, all the weights are equal to one, which leads
to a value of $\omega = 1$ and thus we have
$O(d \log(\mu n))$ as an asymptotic bound for most practical
applications. Even if one encounters a situation, where
$\omega \in O(\operatorname{poly}(n))$, the logarithm assures
that our coreset size is still small.

We now have everything we need to construct our first algorithm.
As we already hinted at, the algorithm will sample the points
from our dataset proportionally to the rounded values values
$s_i'$, i.e. each point is assigned a sampling probability
\begin{equation}
    \label{eq:sampling-probabilities}
    p_i = \frac{s_i'}{S_i'} =
    \frac{\left\lceil \ell_i + \frac{w_i}{\mathcal{W}} \right\rceil_2}
    {\sum_{i=1}^n \left\lceil \ell_i + \frac{w_i}{\mathcal{W}} \right\rceil_2},
\end{equation}
where $\lceil . \rceil_2$ indicates the rounding of $s_i$ such that
$\frac{s_i'}{w_i}$ is a power of two and $\ell_i$ is the statistical
leverage score of the $i$-th datapoint, which can be obtained through
a $QR$ decomposition of the row-wise weighted matrix $\sqrt{D_w}Z$.
We note, that the factor $(80 + 16\mu)$ of the sensitivity
bounds doesn't appear in
Equation~\ref{eq:sampling-probabilities}, because it is the
same for all the sensitivity bounds and thus doesn't influence
the sampling probabilities.
The resulting na\"ive algorithm is given in Algorithm~\ref{algo:naive}.

\begin{algorithm}[t!]
    \DontPrintSemicolon
    \KwIn{Dataset $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$ with
    scaled model matrix $Z \in \mathbb{R}^{n \times d}$
    and weight vector $w \in \mathbb{R}_{>0}^n$,
    $\mathcal{W} = \sum_{i=1}^n w_i$,
    size parameter $k \in \mathbb{N}$}
    \KwOut{A subset $\mathcal{C} \subseteq \mathcal{D}$
        of size $|\mathcal{C}| = k$ with weight vector
        $u \in \mathbb{R}_{>0}^k$}
    Compute the $QR$-decomposition of $\sqrt{D_w}Z = QR$ \;
    \For{$i = 1,...,n$}{
        $\ell_i$ = $\lVert Q_i \rVert_2^2$, where $Q_i$ is the $i$-th row of $Q$ \;
        $a_i = \ell_i + \frac{w_i}{\mathcal{W}}$ \;
        $s_i' = w_i 2^{\left\lceil\log_2\left(\frac{a_i}{w_i}\right)\right\rceil}$ \;
    }
    \For{$i = 1, ..., n$}{
        $p_i$ = $\frac{s_i'}{\sum_{i=1}^n s_i'}$ \;
    }
    $\mathcal{C}^{(0)} = \emptyset$ \;
    \For{$i = 1, ..., k$}{
        Randomly sample a point $c_j$ from $\mathcal{D}$ with probabilities $p_1, ..., p_n$ \;
        $\mathcal{C}^{(i)} = \mathcal{C}^{(i-1)} \cup \{ c_j \}$ \;
        $u_i = \frac{w_j}{k \cdot p_j}$ \;
    }
    \KwRet{$\mathcal{C}^{(k)}$, $(u_1, ..., u_k)^T$}
    \caption{Na\"ive coreset construction algorithm\label{algo:naive}}
\end{algorithm}

Although the na\"ive algorithm is a first proof of concept of how small coresets
for probit regression can in principle be constructed,
there are still multiple issues with this algorithm that
we have to deal with.
The first and most obvious issue is the $QR$ decomposition of
$\sqrt{D_w}Z$, that is needed in order to obtain
the statistical leverage scores.
When the dataset is small and fits into the main
memory, the $QR$ decomposition is not an issue, but as soon as
the size of the data grows, its computation on the whole dataset
becomes infeasible. Ideally, we would like to compute
(or at least approximate) the $QR$ decomposition in a single
pass over the data, i.e. we only want look at each element
of $\mathcal{D}$ once.
Standard algorithms for computing a $QR$ factorization
(see for example the Givens method described in
\cite{matrix-computations}) don't scale well for large datasets,
and usually require more than one pass over the data.
Thus, one of the remaining challenges is to find a way to
adapt the $QR$ decomposition for large datasets in order
to efficiently compute the leverage scores.

The second issue of the na\"ive algorithm lies in the
random sampling procedure. Ideally, we would also like to
perform the sampling according to the distribution defined
by $p_1, ..., p_n$ in a single pass over the dataset.
In the next section, we will further explore how to solve
both of these issues and obtain two algorithms, that only require
two passes or one pass over the data, respectively.
But before we get into that, we conclude this section with a
proof, that the na\"ive algorithm is indeed a correct algorithm
for constructing small coresets for probit regression.

\begin{theorem}
    Algorithm~\ref{algo:naive} returns a $(1 \pm \epsilon)$-coreset
    for probit regression with success probability
    $1 - n^{-c}$ for any absolute constant
    $c > 1$, if the size parameter $k$ satisfies
    \begin{equation*}
        k \in O\left(\frac{\mu d^2}{\epsilon^2} \log(\mu \omega n) \log(\mu d)\right).
    \end{equation*}
\end{theorem}
\begin{proof}
    We use the core theorem of the sensitivity framework
    (see Theorem~\ref{theorem:sensitivity-framework}) in order
    to prove the claim.

    In Lemma~\ref{lemma:sensitivity-bounds},
    we showed that $s_1, ..., s_n$
    are upper bounds on the sensitivities and that
    $S = \sum_{i=1}^n s_i \in O(\mu d)$.
    We also saw, that when rounding the $s_i$ up in such a way that
    $\frac{s_i'}{w_i}$ is a power of two,
    $\sum_{i=1}^n s_i' \leq 2S \in O(\mu d)$.
    Further, we showed that the VC-dimension of the range space
    induced by $F^{\ast '}$ can be upper bounded by
    a term $\Delta \in O(d\log(\mu \omega n))$, where
    $\omega = \frac{w_{max}}{w_{min}}$ is the ratio of the
    largest and smallest weight.
    Setting the failure probability equal to $\delta = n^{-c}$
    for any absolute constant $c > 1$, we can plug everything
    into the core theorem of the sensitivity framework and
    obtain the desired coreset size:
    \begin{align*}
        k & \in O\left( \frac{S'}{\epsilon^2} \left(\Delta \log S' + \log\left(\frac{1}{\delta}\right)\right)\right)  \\
          & \subseteq O\left(\frac{\mu d}{\epsilon^2}\left(d \log(\mu \omega n) \log(\mu d) + \log(n^c\right))\right) \\
          & \subseteq O\left(\frac{\mu d^2}{\epsilon^2}\log(\mu \omega n) \log(\mu d)\right)
    \end{align*}
\end{proof}
