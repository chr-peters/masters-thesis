\subsection{Do small coresets always exist?}

Before attempting to construct an algorithm, that
is able to find small coresets, we first
have to investigate if such a goal is even attainable,
i.e. we have to make sure that small coresets even exist.

Without imposing any restrictions on the datasets, it turns out
that it is not difficult to find a counter example, i.e. to find
a dataset $\mathcal{D}$, such that no subset
$\mathcal{C} \subseteq \mathcal{D}$ of roughly logarithmic size
can be a coreset. This negative result has first been
proven in the context of logistic regression by the
authors of~\cite{on-coresets}, but it turns out that
the problematic dataset that admits no small coresets is
the same for probit regression as well.

This finding forces us to make a decision:
Do we have to give up our search for small coresets because
we now know that they don't always exist?
Or is there still hope, perhaps by imposing some (very reasonable)
restrictions on the class of datasets that we consider?
Before we can turn to this discussion, we first reproduce
the counter example for the general case in the following theorem.

\begin{theorem}
    \label{theorem:index}
    There exists a dataset $\mathcal{D}$ of size
    $|\mathcal{D}| = n$, such
    that any $(1 \pm \epsilon)$-coreset $\mathcal{C}$ of $\mathcal{D}$
    for probit regression has a size $k = |\mathcal{C}|$
    of at least $k \in \Omega\left(\frac{n}{\log{n}}\right)$.
\end{theorem}
\begin{proof}
    We can construct such a  dataset by showing
    how coresets can be used in a
    communication protocol for the so called INDEX game,
    a communication game for two players, Alice and Bob, which
    works like this:

    Alice is given an arbitrary binary string $m \in \{0, 1\}^n$ of $n$ bits
    and Bob is given an index $i \in [n]$.
    The objective of the game is for Alice to send a message to Bob that allows
    Bob to obtain the value $m_i$ of Alice's binary string $m$.
    It was shown in~\cite{index}, that the minimum length of a message
    sent by Alice that still allows Bob to obtain $m_i$ with
    a constant probability that the communication
    protocol doesn't fail is in $\Omega(n)$ bits.
    We will now see, how a coreset for probit regression can be used
    to encode such a message.

    The first step is for Alice to convert her binary string $m$ into
    a dataset $\mathcal{D}$ as follows:
    For each entry $m_j$ of her binary string where $m_j = 1$, she adds
    a point
    \begin{equation*}
        x_j = \left( \cos{\left(2 \pi \frac{j}{n}\right)},
        \sin{\left(2 \pi \frac{j}{n}\right)}, 1 \right)^T
    \end{equation*}
    to her set $\mathcal{D}$ and labels it with $y_j = 1$,
    ending up with the dataset
    \begin{equation*}
        \mathcal{D} = \{(x_j, 1)\}_{j \in \{i \in [n]:\ m_i = 1 \}},
    \end{equation*}
    with all points being on the unit circle.

    The next step for her is to construct a
    $(1 \pm \epsilon)$-coreset $\mathcal{C}$ of $\mathcal{D}$
    for probit regression with sample weights $u \in \mathbb{R}^k_{>0}$
    and to transmit both the coreset and the weight vector to Bob,
    which requires $O(\log(n))$ space for each point and
    weight.
    We will later see, how
    large the size $|\mathcal{C}|=k$ of this coreset must be,
    so that Bob can still use it to
    infer the value of $m_i$.

    As soon as Alice's coreset $\mathcal{C}$ arrives at Bob,
    Bob can use it to obtain the value of $m_i$.
    To do this, Bob first adds two new points
    \begin{equation*}
        q_1 = \left( \cos{\left(2 \pi \frac{i - 0.5}{n}\right)},
        \sin{\left(2 \pi \frac{i - 0.5}{n}\right)}, 1 \right)^T
    \end{equation*}
    and
    \begin{equation*}
        q_2 = \left( \cos{\left(2 \pi \frac{i + 0.5}{n}\right)},
        \sin{\left(2 \pi \frac{i + 0.5}{n}\right)}, 1 \right)^T
    \end{equation*}
    to the set and labels both points with $0$ (see Figure~\ref{fig:index}),
    i.e. Bob now has the dataset
    \begin{equation*}
        \mathcal{C}' = \mathcal{C} \cup \{(q_1, 0)\} \cup \{(q_2, 0)\}.
    \end{equation*}

    Next, he uses this new dataset $\mathcal{C}'$ with
    scaled model matrix $C'$ to
    minimize the weighted objective function
    $f_{C'}^u$ of the probit model,
    by using the Newton-Raphson optimization algorithm.

    Taking a look at Figure~\ref{fig:index}, it becomes evident,
    that Bobs points $q_1$ and $q_2$ are linearly separable from
    the other points if and only if Alice didn't add a point
    $x_i$, i.e. if $m_i = 0$.
    He can use the results of the optimization procedure to
    make a distinction between the two cases,
    which then allows him to determine the value of $m_i$
    like this:

    In the case of $m_i=1$, Bobs points are not linearly separable from
    Alices original points, which means that there must occur at least one
    misclassification at a cost of $g(0) = \log(2)$ for the original
    loss function.
    Because Bobs dataset $\mathcal{C}'$ allows him to obtain a
    $(1 \pm \epsilon)$-approximation of the original cost function, he can
    check if the Newton-Raphson algorithm converges to
    a cost of at least $(1 - \epsilon) \log(2)$.
    In this case, he knows that Alice must have added the point $x_i$,
    which means that $m_i=1$.

    Conversely, if at any point during the optimization procedure
    the cost function drops below
    $(1 - \epsilon) \log(2)$
    and approaches zero, Bob knows that Alice didn't add the point
    $x_i$, because his dataset $\mathcal{C}'$ is linearly separable.
    This will allow him to conclude that $m_i = 0$.

    Let us now see, how large the size $k$ of Alice's coreset must be
    for this protocol to work with constant probability.
    In~\cite{index} it was shown, that the minimum length of a message
    that Alice must send in order for the protocol to work
    is in $\Omega(n)$ bits.
    Since each of the points that Alice created can be encoded in
    $\log(n)$ space, it follows from the lower bound that
    $\Omega(n) \supseteq \Omega(k \log(n))$, so $k$ must be in
    $\Omega\left(\frac{n}{\log(n)}\right)$.

    We can conclude, that if there existed a $(1 \pm \epsilon)$-coreset
    of Alice's dataset $\mathcal{D}$
    for probit regression with size $k \in o\left(\frac{n}{\log(n)}\right)$,
    it would contradict the minimum message length of
    the INDEX communication game, which proves the theorem.
\end{proof}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/index.pdf}
    \caption{Bob places two points $q_1$ and $q_2$ in such a way
        on the unit circle, that they can be linearly seperated from the other
        points if and only if Alice didn't place a point at $x_i$.}
    \label{fig:index}
\end{figure}

We now have an example of a dataset for which no small coresets exist,
which implies that in the general case, without any restrictions,
there are no guarantees that it's even possible to find a small coreset.
But there is one thing that we have to note:
The counter example from the INDEX proof is by no means a
dataset that could ever be reasonably subjected to a probit analysis.
It consists of only positive labels! Further, it is easy to recognize,
that the counter example is linearly separable.
As we already saw in Section~\ref{sec:parameter-estimation}, when
estimating the model parameters, the maximum likelihood estimate
only exists and is unique, when the data is not linearly separable.
So yes, we found an example dataset for which no small coresets exist, but
does that mean that this particular "degenerate" example is relevant
to the attainment of our goal of constructing efficient data reduction
algorithms for the purpose of probit regression? Since the
maximum likelihood estimate doesn't even exist for this dataset,
it can be doubted, to say the least.

It therefore seems reasonable to impose some restrictions on the
datasets under study. Since we are exclusively dealing with
probit regression, it makes sense to restrict the
class of data sets to those, where a probit model can
at least be properly estimated, i.e. where the data is not linearly separable
and were the maximum likelihood estimate exists and is unique.

The authors of~\cite{on-coresets} were dealing with similar issues in
the context of logistic regression, so they decided to introduced a
measure, which they call $\mu$, that describes the degree of separability
of a dataset.
In their work, they were able to not only use this measure to restrict
the class of datasets under study, which they defined as $\mu$-complex,
but also to bound the size of their coresets in terms of $\mu$.
We will go down a similar path in our search for small coresets,
so the first step for us is to slightly adapt this measure
for our purposes:

\begin{definition}($\mu$-complexity)
    \label{def:mu}
    Let $\mathcal{D}$ be a $d$-dimensional dataset of size
    $|\mathcal{D}|=n$ with scaled
    model matrix $Z \in \mathbb{R}^{n \times d}$, where
    $z_i \in \mathbb{R}^d$ constitutes the $i$-th
    row of $Z$ and let
    $w \in \mathbb{R}^n_{>0}$ be a vector of positive weights.
    Let $I_\beta^+ = \{i \in [n]:\ w_i z_i^T \beta > 0 \}$
    and let $I_\beta^- = \{i \in [n]:\ w_i z_i^T \beta < 0 \}$.
    Let
    \begin{equation*}
        \mu_w(\mathcal{D}) = \sup_{\beta \in \mathbb{R}^d \setminus \{0\}}
        \frac{\sum_{i \in I_\beta^+} w_i (z_i^T \beta)^2}
        {\sum_{i \in I_\beta^-} w_i (z_i^T \beta)^2}.
    \end{equation*}
    We call the dataset $\mathcal{D}$ with weight vector $w$
    $\mu$-complex, if there exists a $\mu \in \mathbb{R}$,
    such that $\mu_w(\mathcal{D}) \leq \mu < \infty$.
\end{definition}

In order to understand, what exactly this measure does, we have to
remember that the parameter vector $\beta$ that is estimated in
the probit model is the orthogonal vector of a hyperplane that separates the
space of data points into two partitions.
When using the probit model for binary classification,
the decision to classify a point as positive or negative is often
based on which side of the hyperplane the point is located.
If it is possible to perfectly classify each point such that no
errors are made, the dataset is called linearly separable.
If it isn't linearly separable, there are always two subsets of
datapoints: Those points that are correcly classified and those that
are not. It turns out, that these two subsets are exactly represented
by the sets $I_\beta^+$ and $I_\beta^-$ in the definition of $\mu$.
If we assume without losing generality, that for a given
$\beta \in \mathbb{R}^d$, we classify a point as
positive if $x_i^T \beta < 0$, i.e. it is on the opposite side
of the hyperplane where the normal vector $\beta$ points to,
then $z_i^T \beta = -(2y_i - 1)x_i^T \beta > 0$ and $I_\beta^+$
contains exactly all the indices of correctly classified points.
Conversely, $I_\beta^-$ then contains all the indices of incorrectly
classified points.
It is thus easy to see, that if every point is correctly classified,
i.e. the dataset is linearly separable,
then $I_\beta^- = \emptyset$ and $\mu = \infty$, so the dataset
is not $\mu$-complex.
The relationship between linear separability and
$\mu$-complexity is even stronger though. In the next theorem,
we will show that a finite $\mu$, i.e. the $\mu$-complexity
of a dataset, is exactly equivalent to linear separability.

\begin{theorem}
    \label{theorem:mu-linear-separability}
    Let $\mathcal{D}$ be a $d$-dimensional dataset of size
    $|\mathcal{D}| = n$ like in Definition~\ref{def:mu}
    ($\mu$-complexity)
    and let $w \in \mathbb{R}^n_{>0}$ be a vector of
    positive weights.
    Then, the dataset
    $\mathcal{D}$ with weight vector $w$ is $\mu$-complex
    if and only if $\mathcal{D}$ is not linearly separable.
\end{theorem}
\begin{proof}
We first prove the "$\Rightarrow$" direction, i.e. we show
that if $\mathcal{D}$ is $\mu$-complex,
then it is not linearly separable.
We do this by proving the equivalent
contraposition that if $\mathcal{D}$ is linearly separable,
then it is not $\mu$-complex.

Let $S_0 = \{i \in [n]:\ y_i = 0\}$ and $S_1 = \{i \in [n]:\ y_i = 1\}$
like in Definition~\ref{def:linear-separability}
(linear separability).
If $\mathcal{D}$ is linearly separable, then there exists
a $\beta \in \mathbb{R}^d \setminus \{0\}$, such that
\begingroup
\allowdisplaybreaks
\begin{align*}
                & \forall i \in S_0:\ x_i^T \beta \geq 0\quad \text{and}\quad \forall i \in S_1:\ x_i^T \beta \leq 0                         \\
    \iff        &                                                                                                                            \\
                & \forall i \in S_0:\ (-1) x_i^T \beta \leq 0\quad \text{and}\quad \forall i \in S_1:\ x_i^T \beta \leq 0                    \\
    \iff        &                                                                                                                            \\
                & \forall i \in S_0:\ (2y_i - 1) x_i^T \beta \leq 0\quad \text{and}\quad \forall i \in S_1:\ (2y_i - 1) x_i^T \beta \leq 0   \\
    \iff        &                                                                                                                            \\
                & \forall i \in S_0:\ -(2y_i - 1) x_i^T \beta \geq 0\quad \text{and}\quad \forall i \in S_1:\ -(2y_i - 1) x_i^T \beta \geq 0 \\
    \iff        &                                                                                                                            \\
                & \forall i \in S_0:\ z_i^T \beta \geq 0\quad \text{and}\quad \forall i \in S_1:\ z_i^T \beta \geq 0                         \\
    \iff        &                                                                                                                            \\
                & \forall i \in [n]:\ z_i^T\beta \geq 0                                                                                      \\
    \iff        &                                                                                                                            \\
                & I_\beta^- = \{ i \in [n]:\ w_iz_i^T\beta < 0\} = \emptyset                                                                 \\
    \iff        &                                                                                                                            \\
                & \sum_{i \in I_\beta^-} w_i (z_i^T \beta)^2 = 0                                                                             \\
    \Rightarrow &                                                                                                                            \\
                & \mu_w(\mathcal{D}) \geq \frac{\sum_{i \in I_\beta^+} w_i (z_i^T \beta)^2}
    {\sum_{i \in I_\beta^-} w_i (z_i^T \beta)^2} = \infty.
\end{align*}
\endgroup
We use this notation to indicate,
that in this case, when the dataset is not empty and the
denominator of the fraction is zero, we have that
$\mu_w(\mathcal{D})$ cannot be bounded by any finite $\mu \in \mathbb{R}$,
which means that $\mathcal{D}$ is not $\mu$-complex.

It now remains to prove the "$\Leftarrow$" direction, i.e. to
show that if $\mathcal{D}$ is not linearly separable,
then it is $\mu$-complex. Again, we do this by proving the
equivalent contraposition that if $\mathcal{D}$ is not
$\mu$-complex, then it is linearly separable.

The first step in order to do so is to show that we can restrict the
supremum in $\mu_w(\mathcal{D})$ to finite $\beta$ with
$\lVert \beta \rVert = 1$:
\begin{align*}
    \mu_w(\mathcal{D}) & = \sup_{\beta \in \mathbb{R}^d \setminus \{0\}}
    \frac{\sum_{i \in I_\beta^+} w_i (z_i^T \beta)^2}
    {\sum_{i \in I_\beta^-} w_i (z_i^T \beta)^2}                                         \\ & =
    \sup_{\beta \in \mathbb{R}^d \setminus \{0\}}
    \frac{\sum_{i \in I_\beta^+} \frac{1}{\lVert \beta \rVert^2}w_i (z_i^T \beta)^2}
    {\sum_{i \in I_\beta^-} \frac{1}{\lVert \beta \rVert^2} w_i (z_i^T \beta)^2}         \\
                       & =
    \sup_{\beta \in \mathbb{R}^d \setminus \{0\}}
    \frac{\sum_{i \in I_\beta^+} w_i \left(z_i^T \frac{\beta}{\lVert \beta \rVert}\right)^2}
    {\sum_{i \in I_\beta^-}  w_i \left(z_i^T \frac{\beta}{\lVert \beta \rVert}\right)^2} \\
                       & =
    \sup_{\tilde\beta \in \mathbb{R}^d,\ \lVert \tilde\beta \rVert = 1}
    \frac{\sum_{i \in I_\beta^+} w_i \left(z_i^T \tilde\beta \right)^2}
    {\sum_{i \in I_\beta^-}  w_i \left(z_i^T \tilde\beta \right)^2},
\end{align*}
which lets us conclude that even in the supremum, both expressions
$\sum_{i \in I_\beta^+} w_i (z_i^T \beta )^2$
and
$\sum_{i \in I_\beta^-}  w_i (z_i^T \beta )^2$
are finite.
This means that if $\mathcal{D}$ is not $\mu$-complex, then
the denominator must be zero, i.e. it must hold that there exists
a $\beta \in \mathbb{R}^d \setminus \{0\}$ such that
\begin{equation*}
    \sum_{i \in I_\beta^-}  w_i (z_i^T \beta )^2 = 0.
\end{equation*}

From here, we can follow the same chain of equivalences that we
showed when proving the "$\Rightarrow$"-direction of the theorem,
which leads us directly to the fact, that $\mathcal{D}$ in this case must
be linearly separable, which concludes the proof.
\end{proof}

Having established the relationship between $\mu$-complexity and
linear separability, it directly follows that $\mu$-complexity is
also equivalent to the existence and uniqueness of the
maximum likelihood estimate of the probit model, that we discussed
earlier in Section~\ref{sec:parameter-estimation}.

From now on, we will subject our studies of coresets only to
those datasets, that are $\mu$-complex, i.e. not linearly separable
and with existing and unique maximum likelihood estimate for the
probit model.

We conclude this section by proving some simple inequalities
regarding $\mu$, which will later be helpful when constructing the
coresets.

\begin{lemma}
    \label{lemma:mu-inequalities}
    Let $\mathcal{D}$ be a $d$-dimensional and $\mu$-complex dataset of size
    $|\mathcal{D}|=n$ with scaled
    model matrix $Z \in \mathbb{R}^{n \times d}$ and weight
    vector $w \in \mathbb{R}^n_{>0}$ like in
    Definition~\ref{def:mu}.
    The following relationship holds for all $\beta \in \mathbb{R}^d$:
    \begin{equation*}
        \mu^{-1} \sum_{i \in I_\beta^-} w_i (z_i^T \beta)^2
        \leq \sum_{i \in I_\beta^+} w_i (z_i^T \beta)^2
        \leq \mu \sum_{i \in I_\beta^-} w_i (z_i^T \beta)^2.
    \end{equation*}
\end{lemma}
\begin{proof}
    If $\mathcal{D}$ with weights $w$ is $\mu$-complex, then
    \begin{align*}
             & \frac{\sum_{i \in I_\beta^+} w_i (z_i^T \beta)^2}{\sum_{i \in I_\beta^-} w_i (z_i^T \beta)^2}
        \leq \mu_w(\mathcal{D}) \leq \mu                                                                     \\
        \iff &
        \sum_{i \in I_\beta^+} w_i (z_i^T \beta)^2
        \leq \mu \sum_{i \in I_\beta^-} w_i (z_i^T \beta)^2,
    \end{align*}
    which proves the second inequality.

    Considering that the labeling of a dataset is arbitrary, i.e. we
    could always switch the 1 labels for the 0 labels and vice versa
    (if we flip the sign of $\beta$
    accordingly),
    the following relationship is true as well:
    \begin{align*}
             & \frac{\sum_{i \in I_\beta^-} w_i (z_i^T \beta)^2}{\sum_{i \in I_\beta^+} w_i (z_i^T \beta)^2}
        \leq \mu_w(\mathcal{D}) \leq \mu                                                                     \\
        \iff &
        \sum_{i \in I_\beta^-} w_i (z_i^T \beta)^2
        \leq \mu \sum_{i \in I_\beta^+} w_i (z_i^T \beta)^2                                                  \\
        \iff &
        \mu^{-1} \sum_{i \in I_\beta^-} w_i (z_i^T \beta)^2
        \leq \sum_{i \in I_\beta^+} w_i (z_i^T \beta)^2,
    \end{align*}
    which proves the first inequality.
\end{proof}
