\section{Experiments}

It is now time to investigate, how the two efficient algorithms that
we derived in the previous section behave on real world datasets,
both in the context of maximum likelihood estimation as well as
in a Bayesian setting.
In order to do so, we selected three benchmark datasets in advance
that will be used to compare both algorithms to each other as
well as to the baseline uniform sampling procedure, where each
sample is simply selected with equal probability.

It is important to emphasize, that the datasets used in the evaluation
are not a result of cherry picking: We selected the same datasets
as \cite{on-coresets} as well as \cite{coresets-strengthened}
in order to be comparable to other works in the field, without
testing in advance if our algorithms look good on them or not.

Finally, we note that all of the following experiments were
implemented in Python and executed on an
AMD Ryzen 7 2700x processor with 8 cores of 3.7GHz clock speed
and 16GB of RAM.
The code for all the experiments is open source and can be
found publicly accessible on Github.\footnote{
    \url{https://github.com/cxan96/efficient-probit-regression}
}

\subsection{Datasets}

The datasets that are used in the empirical evaluation of our
algorithms are called Covertype\footnote{
    \url{https://archive.ics.uci.edu/ml/datasets/Covertype}
}, Kddcup\footnote{
    \url{https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html}
} and Webspam\footnote{
    \url{https://www.csie.ntu.edu.tw/\~cjlin/libsvmtools/datasets/binary.html\#webspam}
} and are
all publicly available.

The Covertype dataset consists of $n=581012$ observations of 30x30m
forest patches of wilderness areas located in the
Roosevelt National Forest of Northern Colorado, USA.
The task is to predict the so called covertype of each of these
patches, i.e. the dominant tree species, based on $d=54$
observed features. There are seven distinct tree species that
appear in the dataset, so we have a multiclass problem. To transform
the problem into a binary classification problem that can be
subjected to a probit analysis, we adapted the task to distinguish the
tree species "Lodgepole Pine" from the other six species, thus
obtaining a balanced problem of $49\%$ positive vs $51\%$ negative
observations.
As an additional preprocessing step, all continuous features of the
dataset were scaled to have a mean of zero as well as unit variance.
In addition, an intercept column of all ones was appended to the data.

The Webspam datasets consists of $n=350000$ observations of
web pages, that can either be classified as spam or no spam, based
on the occurence of 254 distinct words on the web page.
$61\%$ of the observations in the dataset are labeled as spam
and the other $39\%$ are labeled as no spam. The task is to predict,
whether a given web page is spam or not.
The features consist of binary $0/1$ observations that indicate, if
a given word is present on a web page or not.
In a preprocessing step, words that are never present
in the dataset were removed, as well
as words that are only present on a single web page.
After preprocessing, the dataset that is subjected to the
experiments consists of $d=128$ binary features.
An intercept column of all ones was appended to the Webspam dataset
as well.

The Kddcup dataset consists of $n=494021$ observations of network
connections, where the task is to predict if a connection is
"good" or "bad", i.e. to distinguish, whether a hacker tried
to gain unauthorized
access to a network or whether someone tried to establish a
normal and authorized connection.
$80\%$ of the connections in the dataset are "bad" and the other
$20\%$ are "good" connections.
In a preprocessing step, $d=33$ continuous features
were retained from the data and scaled to a mean of zero and
unit variance. Just like the other datasets, an intercept column
of all ones was appended to the Kddcup data as well.

\subsubsection{A Visual Comparison of the Datasets}

Before we evaluate the three competing algorithms on each of
the datasets, we first make an attempt at uncovering some of
the structural characteristics of the data, that could
potentially help us to learn more about the situations in which
the algorithms perform well or might fail.

In order to do so, we first project each dataset onto
its first two principal components, which where obtained
as a result of a principal component analysis
\cite{principal-components}.
The first two principal components are orthogonal vectors that
represent the two directions of the data,
in which the highest variance occurs.
By projecting each datapoint onto the subspace spanned by
the first two principal components, we have
a way of visualizing the datasets in a two dimensional space, while
still retaining as much variance as possible, even though we are
reducing the dimensionality to only 2.

Next, we draw two distinct samples from the reduced datapoints,
each consisting of 500 points. The first one is a uniform sample,
where each datapoint is sampled with equal probability.
The second sample is drawn with probabilities that are proportional
to the statistical leverage scores of the observations with
respect to the original dataset, without applying a PCA.
Both of these samples are presented for each of the datasets in
figure~\ref{fig:dataset-comparison}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.49\linewidth]{figures/covertype_pca.pdf}
    \includegraphics[width=.49\linewidth]{figures/kddcup_pca.pdf}
    \includegraphics[width=.49\linewidth]{figures/webspam_pca.pdf}
    \includegraphics[width=.49\linewidth]{figures/webspam_pca_zoomed.pdf}
    \caption{The datasets are compared by projecting the datapoints
        onto the first two principal components and then drawing
        a random sample of 500 points (a) uniformly and (b) proportionally to the
        statistical leverage scores of the original data.}
    \label{fig:dataset-comparison}
\end{figure}

We can see, that the samples reveal some interesting characteristics
of the datasets. Starting with Covertype, we notice that there
hardly seems to be any difference between the uniform sample and
the leverage score sample. Taking into consideration, that the
statistical leverage scores assign a higher importance to
outliers or "unusual" datapoints, we can conclude that there don't
seem to be too many unusual observations within the Covertype
dataset, or else the two samples would differ more substantially.
It seems to be the case, that a uniform sample is already
sufficient to capture most of the inherent structure of the data,
without having to put extra weight on unusual datapoints, which
simply appear not to exist in this dataset.
Thus, from now on, it seems reasonable to think of the
Covertype dataset as being
a representative for such situations, where the data is
already quite uniformly distributed and not a lot of outliers are
present, i.e. where a uniform sample already seems to
yield good results.

In contrast to Covertype, the Kddcup dataset exhibits a different
picture. Here, we can see that the uniform sample and the leverage
score sample differ way more substantially than it was the case
for the Covertype dataset.
While the uniform sample only seems to occupy a very limited portion
of the feature space, most of it being a cluster of
many points close to the origin,
the leverage score sample covers a lot more
of the feature space,
indicating that there are a variety of unusual datapoints,
or outliers present,
which the uniform sample misses.
Thus, it seems that we can think of the Kddcup dataset as an
example of a situation,
where on the one hand, we have a lot of similar observations that
are concentrated within a comparatively small proportion of
the feature space, but on the other hand, we also have a
considerable amount of unique observations, which are scattered
around the feature space
while seemingly not belonging to any particular cluster.

Lastly, another interesting situation is presented to us when taking
a look at the two samples of the Webspam dataset.
What strikes the eye first, is that there seem to be some truly
extreme outliers present in the dataset, that the leverage score
sampling picked up on. Only by zooming in on the more crowded area
of the feature space and thereby ignoring those extreme outliers,
we can see the remaining parts of the two samples, which seem to
be rather similar, just like in the situation of the Covertype dataset.
It thus seems to be the case, that the Webspam dataset is an example
of a situation, where on the one hand, the vast majority of the
datapoints are concentrated around some region of the feature space,
but on the other hand, there are also a few hard hitting outliers present,
which exhibit enormous differences to the majority of the other
observations.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.49\linewidth]{figures/covertype_ratio_plot.pdf}
    \includegraphics[width=.49\linewidth]{figures/webspam_ratio_plot.pdf}
    \includegraphics[width=.49\linewidth]{figures/kddcup_ratio_plot.pdf}
    \includegraphics[width=.49\linewidth]{figures/kddcup_ratio_plot_zoomed.pdf}
    \caption{TODO.}
    \label{fig:ratio-plots}
\end{figure}