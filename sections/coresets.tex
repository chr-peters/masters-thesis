\section{Coresets and Sensitivity Sampling}

The Newton-Raphson algorithm for optimizing the
objective function of the probit model,
as well as the Gibbs sampler, are reasonably efficient
when the datasets are of small to moderate size.
Usually, this is the case when it's possible to
store the model matrix $X$ into the main memory.
But problems arise, when the datasets are getting
so big, that this is no longer possible.
What should we do in such a case?

One idea to deal with this issue is that we could
select a smaller subset $\mathcal{C}$ of our initial dataset
$\mathcal{D}$, that represents the characteristics of
the original data well in some sense.
We hope, that when we execute the computationally expensive
optimization algorithms on the smaller subset,
we still get similar results as if we executed the
algorithms on the original dataset.
But what does it mean for a subset $\mathcal{C}$
to be representative of $\mathcal{D}$?
And how could we come up with an algorithm
that selects such a subset efficiently?
The method of \textit{coresets}
(see for example~\cite{munteanu-coresets-introduction}) is one way
of dealing with these questions, which we will explore in this chapter.

So, what is a coreset? As the name suggests, we are
talking about a subset $\mathcal{C} \subseteq \mathcal{D}$
of our initial dataset, that fulfills some very special
requirements which ensure that the original
dataset is well represented for our problem.
To be more specific, we want the objective function
$f(\beta)$ evaluated on the coreset to be
as close to the objective function on the original dataset as possible,
for all $\beta \in \mathbb{R}^d$.
Mathematically speaking, what we are interested in is
a so-called $(1 \pm \epsilon)$ approximation of the
objective function on the original dataset.

To understand what is meant by that, assume for a moment
that we are given a function $f(\beta)$ and an
approximation $\tilde f(\beta)$.
If $\tilde f(\beta)$ is a $(1 \pm \epsilon)$ approximation
of $f(\beta)$, it will never deviate from $f(\beta)$
more than a factor $(1 \pm \epsilon)$, i.e.
we have for all $\beta \in \mathbb{R}^d$, that:
\begin{equation*}
    (1 - \epsilon) f(\beta) \leq \tilde f(\beta) \leq (1 + \epsilon) f(\beta).
\end{equation*}
This kind of approximation would then allow us to run
an optimization algorithm on $\tilde f(\beta)$ and
guarantee that our solution is close to the
optimal solution on $f(\beta)$.

As we already hinted at, a coreset is simply a subset
$\mathcal{C} \subseteq \mathcal{D}$ of our original dataset,
that provides us with a $(1 \pm \epsilon)$ approximation
of the original loss function.
We formalize this concept in the following definition.

\begin{definition}[Coreset]
    \label{def:coreset}
    Let $\mathcal{D}=\{(x_i, y_i)\}_{i=1}^n$ be a $d$-dimensional dataset
    with scaled model matrix $Z \in \mathbb{R}^{n \times d}$,
    i.e. $z_i = -(2y_i - 1)x_i$ constitutes the $i$-th
    row of Z, and let $w \in \mathbb{R}_{>0}^n$ be
    a vector of positive sample weights.
    Let $\mathcal{C} \subseteq \mathcal{D}$ be a subset of $\mathcal{D}$
    of size $|\mathcal{C}| = k$
    with scaled model matrix $C \in \mathbb{R}^{k \times d}$ and
    a vector of positive sample weights $u \in \mathbb{R}_{>0}^k$.
    Let $\epsilon > 0$.
    We call $\mathcal{C}$ a $(1 \pm \epsilon)$-coreset of $\mathcal{D}$
    for probit regression, if
    \begin{equation*}
        (1-\epsilon)f_Z^w(\beta) \leq f_C^u(\beta) \leq (1+\epsilon)f_Z^w(\beta)
        \quad \forall \beta \in \mathbb{R}^d,
    \end{equation*}
    where $f_Z^w(\beta) = \sum_{i=1}^n w_i g(z_i^T \beta)$ is the
    weighted objective function of the probit model
    and $g(x) = \ln \left( \frac{1}{1 - \Phi(x)} \right)$ is the
    probit loss.
\end{definition}

One thing to note here is that we do not only need to select the
subset $\mathcal{C} \subseteq \mathcal{D}$, but we also have to come
up with some new sample weights $u$.
Intuitively speaking, this makes sense because when reducing the
amount of data points in the objective function, which is
achieved by selecting the subset $\mathcal{C}$, we are
also naturally lowering its overall value, since $g$, the
probit loss, is a positive function. The reweighting
by $u$ accounts for that, so we can still get a $(1 \pm \epsilon)$
approximation of the original loss function.

Let us now consider perhaps the most important aspect of this definition,
which will determine the usefullness of \textit{any} work in the
domain of coresets: The coreset size $k = |\mathcal{C}|$.
It can easily be verified, that we can always come up with a coreset
when $k = n$, i.e. the so-called trivial coreset, where
we simply select $\mathcal{C} = \mathcal{D}$.
But such a coreset doesn't help us at all with our goal of reducing the
computational burden of the optimization and Gibbs sampling algorithms.
Informally speaking, we want $k$ to be small. But how small
is small enough? Usually, we can consider it a success, if we
can find coresets where $k \in O(\log(n))$, using the big-o notation
to indicate that $k$ is not much larger than the logarithm
of the amount of data points. If, for example, we had
a dataset with one billion observations, i.e. $n = 1,000,000,000$,
the natural logarithm of n would equal to roughly 20 datapoints.
That sounds like a decent compression, doesn't it?

In the remainder of this work, we will refer to a coreset as
small, if $k$ is roughly logarithmic in $n$.
Our goal is to construct algorithms, which will enable us to
find such small coresets in the context of probit regression.

\input{sections/coresets/lower_bounds_mu}

\input{sections/coresets/sensitivity_framework}

\subsection{Constructing the Coreset}

\input{sections/coresets/sensitivity_bounds}

\input{sections/coresets/vc_dimension_bounds}

\input{sections/coresets/algorithm}

