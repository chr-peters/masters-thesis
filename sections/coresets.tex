\section{Coresets and Sensitivity Sampling}

In this work, we are using the method of coresets
(see for example~\cite{munteanu-coresets-introduction})
to approach the problem of data reduction for the probit model.
The idea behind coresets is, that when given a
dataset $\mathcal{D}$, we are interested in selecting
only a small subset of observations
$\mathcal{C} \subseteq \mathcal{D}$, such that the objective
function evaluated on the (possible reweighted) subset $\mathcal{C}$
does not differ too much
from the objective function evaluated on the original dataset $\mathcal{D}$.

This approach will allow us to estimate the model parameters
efficiently on the ideally much smaller set $\mathcal{C}$,
when a full optimization on $\mathcal{D}$ could already
be infeasible for big datasets.
We are thus following the paradigm of \textit{sketch-and-solve},
i.e. first reducing the size of the original dataset and then solving
the optimization problem on the reduced dataset.

In order to work out a formal definition of when we call a subset
$\mathcal{C} \subseteq \mathcal{D}$ a coreset in the context of
probit regression, we first have to slightly extend the
concept of the model matrix, as we will need it for the coreset
definition.

\begin{definition}[Scaled model matrix]
    Let $\mathcal{D}=\{(x_i, y_i)\}_{i=1}^n$ be a $d$-dimensional dataset.
    Let $z_i = (2y_i - 1)x_i$ for all i in $[n]$.
    Then we call the matrix $Z \in \mathbb{R}^{n \times d}$, where the
    $i$-th row consists of the vector $z_i$ for all $i \in [n]$,
    the scaled model matrix of $\mathcal{D}$.
\end{definition}

This definition of the scaled model matrix is nothing particularly new,
it just formalizes the concept of factoring the labels into the
model matrix, which we already encountered when dealing with the
parameter estimation in section~\ref{sec:parameter-estimation}.

We are now ready for the coreset definition:

\begin{definition}[Coreset]
    \label{def:coreset}
    Let $\mathcal{D}=\{(x_i, y_i)\}_{i=1}^n$ be a $d$-dimensional dataset
    with scaled model matrix $Z \in \mathbb{R}^{n \times d}$ and
    a vector of positive sample weights $w \in \mathbb{R}_{>0}^n$.
    Let $\mathcal{C} \subseteq \mathcal{D}$ be a subset of $\mathcal{D}$
    of size $|\mathcal{C}| = k$
    with scaled model matrix $C \in \mathbb{R}^{k \times d}$ and
    a vector of positive sample weights $u \in \mathbb{R}_{>0}^k$.
    Let $\frac{1}{2} > \epsilon > 0$.
    We call $\mathcal{C}$ a $(1+\epsilon)$-coreset of $\mathcal{D}$
    for probit regression, if
    \begin{equation*}
        (1-\epsilon)f_Z^w(\beta) \leq f_C^u(\beta) \leq (1+\epsilon)f_Z^w(\beta)
        \quad \forall \beta \in \mathbb{R}^d,
    \end{equation*}
    where $f_Z^w(\beta) = \sum_{i=1}^n w_i g(z_i^T \beta)$ is the
    weighted objective function of the probit model.
\end{definition}

The size parameter $k = |\mathcal{C}|$ of a coreset usually depends
on the desired approximation quality $\epsilon$, as well as on
specific problem characteristics, such as the number of observations
$n$ as well as the dimensionality $d$ of the dataset.
When constructing coresets, we are interested in keeping this parameter
low in comparison to the total size of the dataset, i.e. we
usually require that at least $k \in O(\log{n})$, so that
the data reduction is actually meaningful.

In the next section, we will investigate if there are any
guarantees that can be given regarding the coreset size
without imposing any further restrictions on the dataset.
We will find out, that in the general case, it can't
be guaranteed that a reasonably small coreset always exists.
As a consequence, we will later confine our research to a
specific class of datasets that we will call $\mu$-complex,
for which small upper bounds on the coreset size can be
derived.

\input{sections/coresets/lower_bounds_mu}

\input{sections/coresets/sensitivity_framework}

\subsection{Constructing the Coreset}

\input{sections/coresets/sensitivity_bounds}

\input{sections/coresets/vc_dimension_bounds}

\subsubsection{A simple two-pass algorithm}

\begin{theorem}
    Let $\mathcal{D}$ be a $d$-dimensional and $\mu$-complex dataset of
    size $|\mathcal{D}|=n$ with scaled model matrix
    $Z \in \mathbb{R}^{n \times d}$, let $w \in \mathbb{R}^n_{>0}$ be
    a vector of positive weights, with
    $\omega = \frac{w_{max}}{w_{min}}$ being the ratio of the largest and
    smallest weight, $\mathcal{W} = \sum_{i=1}^n w_i$ being the
    sum of all weights, and let $U \in \mathbb{R}^{n \times d}$
    be an orthonormal basis for the columnspace of
    $\sqrt{D_wZ}$, where $U_i \in \mathbb{R}^d$ is the vector that
    constitutes the $i$-th row of $U$. Let
    $\epsilon \in (0, \frac{1}{2})$.

    If $\mathcal{C} \subseteq \mathcal{D}$ is a subset of $\mathcal{D}$
    of size $|\mathcal{C}| = k$, that was obtained by independently sampling
    \begin{equation*}
        k \in O\left(\frac{\mu d^2}{\epsilon^2} \log(\omega n) \log(\mu d)\right)
    \end{equation*}
    elements from $\mathcal{D}$ proportional to
    \begin{equation*}
        q_i = \min\left\{ 2^l\ |\ l \in \mathbb{Z},\  2^l \geq \lVert U_i \rVert_2^2 + \frac{w_i}{\mathcal{W}} \right\},
    \end{equation*}
    i.e. with sampling probability $p_i = \frac{q_i}{\sum_{i=1}^n q_i}$
    for all $i \in [n]$ and
    $u \in \mathbb{R}^k_{>0}$ is a new weight vector, where
    $u_j = \frac{w_i \sum_{l=1}^n p_l}{kp_i}$ is the new weight for
    an element in $\mathcal{C}$ that corresponds to the $i$-th element
    of $\mathcal{D}$,
    then with probability $1 - \log^{-c}(n)$, $\mathcal{C}$ with weights $u$
    is a $(1 \pm \epsilon)$-coreset of $\mathcal{D}$ for probit regression
    for any absolute constant $c > 1$.
\end{theorem}
\begin{proof}
    $S \leq 2 \cdot 192 \mu d$, $\Delta = d\log(\omega n)$, $\delta = \log^{-c}(n)$.
    \begin{align*}
        k & \in O\left( \frac{S}{\epsilon^2} \left(\Delta \log S + \log\left(\frac{1}{\delta}\right)\right)\right)                  \\
          & \subseteq O\left(\frac{\mu d}{\epsilon^2}\left(d \log(\omega n) \log(\mu d) + \log\left(\log^c(n)\right) \right)\right) \\
          & \subseteq O\left(\frac{\mu d^2}{\epsilon^2}\log(\omega n) \log(\mu d)\right)
    \end{align*}
\end{proof}
