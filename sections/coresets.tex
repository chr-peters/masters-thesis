\section{Coresets}

\begin{definition}
    Let $Z \in \mathbb{R}^{n \times d}$ be a set of points weighted by
    $w \in \mathbb{R}^n_{>0}$.
    Then a set $C \in \mathbb{R}^{k \times d}$ weighted
    by $u \in \mathbb{R}^k_{>0}$ is a $(1 \pm \epsilon)$-coreset
    of $Z$ for $f_{w, Z} = \sum_{i=1}^n w_i g(z_i \beta)$ if
    \begin{equation*}
        \ (1 - \epsilon)f_{w, Z}(\beta)
        \leq f_{u, C}(\beta) \leq (1 + \epsilon)f_{w, Z}(\beta)
        \quad \forall \beta \in \mathbb{R}^d,
    \end{equation*}
    where $f_{u, C} = \sum_{i=1}^k u_i g(c_i \beta)$.
\end{definition}

\subsection{Lower Bounds}

\begin{theorem}
    Let $X \in \mathbb{R}^{n \times 2}$, $y \in \{-1, 1\}^n$ be an
    instance of probit regression.
    Any coreset of $X$, $y$ for probit regression consists of
    at least $\Omega\left(\frac{n}{\log{n}}\right)$ points.
\end{theorem}
\begin{proof}
    We first show how such a coreset could be used in a
    communication protocol for the INDEX communication game
    to transmit a message.
    Since there exists a lower bound on the minimum
    message length of the INDEX game (see~\cite{index}),
    we can use it to derive a lower bound on the
    coreset size.
    The same technique was also used in~\cite{on-coresets} to find
    lower bounds for coresets of logistic regression and is here slightly
    adapted for probit regression.

    The INDEX game consists of two players, Alice and Bob.
    Alice is given a random binary string $x \in \{0, 1\}^n$ of $n$ bits
    and Bob is given an index $i \in [n]$.
    The goal is for Alice to send a message to Bob that allows
    Bob to obtain the value $x_i$ of Alice's binary string $x$.
    It was shown in~\cite{index}, that the minimum length of a message
    sent by Alice that still allows Bob to obtain $x_i$ with
    constant probability is in $\Omega(n)$ bits.
    We will now see how a coreset for probit regression can be used
    to encode such a message.

    The first step is for Alice to convert her binary string $x$ into
    a set $P$ of two-dimensional points as follows:
    For each entry $x_j$ of her binary string where $x_j = 1$, she adds
    a point $p_j = \left( \cos{\left(\frac{j}{n}\right)},
        \sin{\left(\frac{j}{n}\right)} \right)$
    to her set $P$ and labels it with $1$.
    As we can see, all of these points are on the unit circle and all
    of them are labeled with $1$.
    Next, she uses these points to construct a coreset for probit regression
    $C \in \mathbb{R}^{k \times 2}$ of $P$
    and sends it to Bob. We will later see, how
    large the size $k$ of this coreset must be, so that Bob can still
    obtain $x_i$ with constant probability.

    As soon as Alice's coreset $C$ arrives at Bob, Bob can use it to
    obtain the value of $x_i$.
    To do this, Bob first adds a new point $p_i = (1 - \delta)
        \left( \cos{\left(\frac{j}{n}\right)},
        \sin{\left(\frac{j}{n}\right)} \right)$
    for some small $\delta > 0$
    to the set and labels it with $-1$.
    Next, he uses his point $p_i$ together with the coreset $C$ to
    obtain a solution for the corresponding probit regression problem.
    He can then use the value of the cost function to determine the value
    of $x_i$ like this:

    Since Alice only added a point $p_j$ to her set if $x_j = 1$, his
    new point $p_i$ is linearly seperable from Alice's points if
    the value of $x_i = 0$, i.e. Alice didn't add a point for $x_i$.
    In this case, the value of the cost function tends to zero.
    If on the other hand, Bob's new point $p_i$ can't be linearly
    seperated from the other points, it means that Alice added a point
    for $x_i = 1$. In this case, there must be at least one
    misclassification and the value of the cost function is at least
    $g(0) = \log(2)$.
    Since coresets can be used to obtain $(1 + \epsilon)$-approximation
    of the objective function, Bob can use this case distinction to
    determine the value of $x_i$.

    Let us now see how large the size $k$ of Alice's coreset must be
    for this protocol to work with constant probability.
    In~\cite{index} it was shown, that the minimum length of a message
    that Alice can send is in $\Omega(n)$ bits.
    Since each of the points that Alice created can be encoded in
    $\log(n)$ space, it follows from the lower bound that
    $\Omega(n) \subseteq \Omega(k \log(n))$, so $k$ must be in
    $\Omega\left(\frac{n}{\log(n)}\right)$.

    We can conclude that if there existed a $(1 + \epsilon)$-coreset
    for probit regression with size $k \in o\left(\frac{n}{\log(n)}\right)$
    it would contradict the minimum message length of
    INDEX which proves the claim.
\end{proof}