\section{Notes}

\subsection{Online Leverage Scores}

The leverage scores of a matrix $A \in \mathbb{R}^{n \times d}$
are given by $l_i = a_i^T (A^TA)^{-1}a_i$~\cite{online-row-sampling}.
According to~\cite{online-row-sampling}, we can obtain overestimates
of these scores by using only a subset of the rows in $A$ to compute them.

Let $A_j$ be a matrix that contains only the first $j$ rows of $A$.
It follows that the the estimated leverage score
$\tilde{l}_j = a_j^T (A_j^TA_j)^{-1}a_j$ is an overestimate of $l_j$.
In a recent paper by~\cite{tensor-factorization}, it was shown that
the sum of these overestimates can be bounded regardless of how
the rows in $A$ are ordered:
\begin{lemma}[\cite{tensor-factorization}]
    \begin{equation*}
        \sum_{i=1}^n \tilde{l}_j \in O(d + d \log \lVert A \rVert - \min_{i \in [n]} \lVert a_i \rVert)
    \end{equation*}
\end{lemma}

Next, we show how a simple algorithm that computes $\tilde{l}_j$ in an
online manner (passing row by row over the data stream)
can be constructed requiring only $\mathcal{O}(d^2)$ of working memory.
The idea is to only keep the matrix $A_j^TA_j \in \mathbb{R}^{d \times d}$
in memory and update it
for every new row $a_{j+1}$ using a rank one update
$A_{j+1}^TA_{j+1} = A_j^TA_j + a_{j+1} \cdot a_{j+1}^T$.
See~\cite{matrix-computations} for more on matrix multiplication using
outer products. The algorithm is given in algorithm~\ref{algo:online-leverage-scores}.

\begin{algorithm}
    \DontPrintSemicolon
    \KwIn{Matrix $A \in \mathbb{R}^{n \times d}$}
    \KwOut{Online leverage scores $\tilde{l}_i$ for all $i \in [n]$}
    Initialize $M_0 = 0^{d \times d}$ \;
    \ForEach{$a_i := i\textup{'th row vector of }A$, $a_i \in \mathbb{R}^d$}{
        $M_i = M_{i-1} + a_i \cdot a_i^T$ \;
        $\tilde{l}_i = a_i^T M_i^\dagger a_i$ \;
    }
    \KwRet{$\tilde{l}_i,\ i \in [n]$}
    \caption{Online Leverage Scores\label{algo:online-leverage-scores}}
\end{algorithm}
