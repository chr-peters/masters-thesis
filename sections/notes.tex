\section{Notes}

\newpage

\subsection{VC Dimension}

An alternative approach is to write down the VC dimension by using
an instance space and a concept class as given
in \cite{computational-learning-theory}.

\begin{lemma}
    Let $X = \left\{ x_1, ..., x_n \right\} \subset \mathbb{R}^d \times \mathbb{R}_{>0}$
    be the instance space consisting of $n$ points with their last
    coordinate being positive.
    The concept class of interest, $\mathcal{C}$ over $X$, is given as follows:
    \begin{equation*}
        \mathcal{C} = \left\{ \{x \in X:\ f_{\beta, r}(x) \geq 0 \} \ |\ \beta \in \mathbb{R}^d, r \geq 0 \right\},
    \end{equation*}
    with
    \begin{equation*}
        f_{\beta, r}(x) = x_{d+1} \cdot g\left(\sum_{i=1}^d x_i \beta_i\right) - r
    \end{equation*}
    and
    \begin{equation*}
        g(x) = -\log \Phi(-x).
    \end{equation*}
    The VC dimension of $\mathcal{C}$ is equal to the VC dimension of the
    range space induced by
    $\mathcal{F}^w_{probit} = \{ w_i g(z_i \beta) \ | \ i \in [n] \}$,
    $Z \in \mathbb{R}^{n \times d}$, $w \in \mathbb{R}^n_{>0}$.
\end{lemma}

There a few different strategies that can be used to find an upper
bound on the VC dimension of $\mathcal{C}$, as shown by the following
lemmas.
The first one is a simple upper bound for finite concept classes:

\begin{lemma}
    Let $X$ be an instance space and $\mathcal{C}$ be a concept class
    over $X$.
    If the cardinality of $\mathcal{C}$ can be bounded by $m$, i.e.
    $|\mathcal{C}| \leq m$, then $VCdim(\mathcal{C}) \leq log(m)$.
\end{lemma}

The next lemma partitions the concept class into smaller classes,
for each of which the VC dimension can be bounded:

\begin{lemma}
    Let $X$ be an instance space and $\mathcal{C}$ be a concept class
    over $X$.
    Let $\mathcal{C}_1, ..., \mathcal{C}_k$ be a partition of $\mathcal{C}$
    into $k$ disjoint subsets, i.e.
    $\mathcal{C} = \bigcup_{i=1}^k \mathcal{C}_i$ and
    $\mathcal{C}_i \cap \mathcal{C}_j = \emptyset\ \forall i \neq j$.
    Then, $VCdim(\mathcal{C}) \leq \sum_{i=1}^k VCdim(\mathcal{C}_i)$.
\end{lemma}
\begin{proof}
    For the sake of contradiction, assume there was a set $S \subseteq X$
    of size $|S| > \sum_{i=1}^k VCdim(\mathcal{C}_i)$ that is shattered by
    $\mathcal{C}$. If $S$ is shattered by $C$, every subset of $S$ must also
    be shattered by $C$.
    Consider the intersections $T_i = \bigcup_{c \in \mathcal{C}_i} S \cap c$.
    Every $T_i$
    is a subset of $S$ and $S = \bigcup_{i=1}^k T_i$.
    Since $S$ is shattered by $\mathcal{C}$,
    every $T_i$ must be shattered by $\mathcal{C}_i$.
    We assumed that $|S| > \sum_{i=1}^k VCdim(\mathcal{C}_i)$.
    It follows that there exists a $T_j$ with $T_j > VCdim(\mathcal{C}_j)$.
    Since $T_j$ is also shattered by $\mathcal{C}_j$, this is a contradiction,
    which concludes the proof.
\end{proof}

A result in~\cite{vc-dimension-partition} suggests an even smaller upper bound:
\begin{lemma}[\cite{vc-dimension-partition}]
    Let $X$ be an instance space and $\mathcal{C}$ be a
    concept class over $X$.
    Let $\mathcal{C} = \bigcup_{i=1}^k \mathcal{C}_i$
    and $VCdim(\mathcal{C}_i) \leq m$.
    If $k$ is bounded by a polynomial function of $m$,
    then $VCdim(\mathcal{C}) \leq 3m$.
\end{lemma}

Instead of partitioning the concept class, we could also partition the
instance space and obtain a similar bound:

\begin{lemma}
    \label{lemma:instance-space-partition}
    Let $X$ be an instance space and $\mathcal{C}$ be a concept class
    over $X$.
    Let $X_1, ..., X_k$ be a partition of $X$ into $k$ disjoint subsets, i.e.
    $X = \bigcup_{i=1}^{k} X_i$ and
    $X_i \cap X_j = \emptyset\ \forall i \neq j$.
    Let $\mathcal{C}_i = \left\{ X_i \cap c\ |\ c \in \mathcal{C} \right\}$
    be a concept class over $X_i$ for all $i \in [k]$.\\
    Then, $VCdim(\mathcal{C}) \leq \sum_{i=1}^k VCdim(\mathcal{C}_i)$.
\end{lemma}
\begin{proof}
    Again, assume there existed a set $S \subseteq X$ of size
    $|S| > \sum_{i=1}^k VCdim(\mathcal{C}_i)$ that is shattered
    by $\mathcal{C}$.
    $S$ can be partitioned into disjoined subsets $T_i = S \cap X_i$, with
    $\bigcup_{i=1}^k T_i = S$.
    Every $T_i$ must be shattered by $\mathcal{C}_i$.
    Since we assumed that $|S| > \sum_{i=1}^k VCdim(\mathcal{C}_i)$,
    there exists a $T_j$ with $|T_j| > VCdim(C_j)$ which is also
    shattered by $C_j$. This contradiction concludes the proof.
\end{proof}

\newpage

\subsection{New idea for VC dimension proof}

\begin{lemma}
    Let
    \begin{equation*}
        h_{\beta, r}(x) =
        \begin{cases}
            1 \ \textup{if} \quad x_{d+1} \cdot g\left( \sum_{i=1}^d x_i \beta_i \right) - r \geq 0 \\
            0 \ \textup{else}
        \end{cases}
    \end{equation*}
    Be a function from $\mathbb{R}^{d+1}$ to $\{0, 1\}$ with parameters
    $\beta \in \mathbb{R}^d$ and $r \in \mathbb{R}_{\geq 0}$ with
    \begin{equation*}
        g(x) = \log\left( \frac{1}{1 - \Phi(x)}\right),
    \end{equation*}
    \begin{equation*}
        \Phi(x) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^x e^{-\frac{1}{2}z^2}dz\ .
    \end{equation*}
    Let
    \begin{equation*}
        H = \left\{ x \mapsto h_{\beta, r}(x) \ |\ \beta \in \mathbb{R}^{d},\ r \in \mathbb{R}_{\geq 0} \right\}
    \end{equation*}
    be the hypothesis class determined by $h$. Then, the VC dimension of $H$ is \ldots
\end{lemma}
\begin{proof}
    Let $S = \sum_{i=1}^d x_i \beta_i$. We show that $h$ can be computed in
    $t$ steps as follows:
    \begin{gather*}
        x_{d+1} \cdot g\left( S \right) - r                            \geq 0                                      \\
        \iff  \log\left( \frac{1}{1 - \Phi(S)}\right)                        \geq \frac{r}{x_{d+1}}                      \\
        \iff  \frac{1}{1 - \Phi(S)}                                          \geq \exp\left(\frac{r}{x_{d+1}}\right)     \\
        \iff  1 - \Phi(S)                                                    \leq \exp\left(-\frac{r}{x_{d+1}}\right)    \\
        \iff  \Phi(S)                                                        \geq 1 -\exp\left(-\frac{r}{x_{d+1}}\right) \\
        \iff  \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^S e^{-\frac{1}{2}z^2}dz  \geq 1 -\exp\left(-\frac{r}{x_{d+1}}\right) \\
        \iff   \int_{-\infty}^S e^{-\frac{1}{2}z^2}dz  \geq \sqrt{2 \pi}\left(1 -\exp\left(-\frac{r}{x_{d+1}}\right)\right) \\
    \end{gather*}
\end{proof}

\newpage

\subsection{Online Leverage Scores}

The leverage scores of a matrix $A \in \mathbb{R}^{n \times d}$
are given by $l_i = a_i^T (A^TA)^{-1}a_i$~\cite{online-row-sampling}.
According to~\cite{online-row-sampling}, we can obtain overestimates
of these scores by using only a subset of the rows in $A$ to compute them.

Let $A_j$ be a matrix that contains only the first $j$ rows of $A$.
It follows that the the estimated leverage score
$\tilde{l}_j = a_j^T (A_j^TA_j)^{-1}a_j$ is an overestimate of $l_j$.
In a recent paper by~\cite{tensor-factorization}, it was shown that
the sum of these overestimates can be bounded regardless of how
the rows in $A$ are ordered:
\begin{lemma}[\cite{tensor-factorization}]
    \begin{equation*}
        \sum_{i=1}^n \tilde{l}_j \in O(d + d \log \lVert A \rVert - \min_{i \in [n]} \lVert a_i \rVert)
    \end{equation*}
\end{lemma}

Next, we show how a simple algorithm that computes $\tilde{l}_j$ in an
online manner (passing row by row over the data stream)
can be constructed requiring only $\mathcal{O}(d^2)$ of working memory.
The idea is to only keep the matrix $A_j^TA_j \in \mathbb{R}^{d \times d}$
in memory and update it
for every new row $a_{j+1}$ using a rank one update
$A_{j+1}^TA_{j+1} = A_j^TA_j + a_{j+1} \cdot a_{j+1}^T$.
See~\cite{matrix-computations} for more on matrix multiplication using
outer products. The algorithm is given in algorithm~\ref{algo:online-leverage-scores}.

\begin{algorithm}
    \DontPrintSemicolon
    \KwIn{Matrix $A \in \mathbb{R}^{n \times d}$}
    \KwOut{Online leverage scores $\tilde{l}_i$ for all $i \in [n]$}
    Initialize $M_0 = 0^{d \times d}$ \;
    \ForEach{$a_i := i\textup{'th row vector of }A$, $a_i \in \mathbb{R}^d$}{
        $M_i = M_{i-1} + a_i \cdot a_i^T$ \;
        $\tilde{l}_i = a_i^T M_i^\dagger a_i$ \;
    }
    \KwRet{$\tilde{l}_i,\ i \in [n]$}
    \caption{Online Leverage Scores\label{algo:online-leverage-scores}}
\end{algorithm}

\section{Probit Regression}

\paragraph{Situation:}
We have $n$ data points $(x_i, y_i), \ i=1,...,n$ with
$x_i \in \mathbb{R}^d$ and $y_i \in \{ -1, 1\}$.

\paragraph{Probit Model:}
$y_i$ is a realization of the random variable $Y_i$.
$Y_1, ..., Y_n$ are independent.
The distribution of $Y_i$ is as follows:
\begin{align*}
    P(Y_i = 1 | x_i; \beta)  & = \Phi(x_i^T \beta)                          \\
    P(Y_i = -1 | x_i; \beta) & = 1 - \Phi(x_i^T \beta) = \Phi(-x_i^T \beta)
\end{align*}
where $\beta \in \mathbb{R}^d$.
It follows that
\begin{align*}
    P(Y_i = y_i | x_i; \beta) = \Phi(y_i x_i^T \beta)
\end{align*}

\paragraph{Likelihood:}
The likelihood of a parameter vector $\beta$ is given as follows:
\begin{equation*}
    L(\beta) = \prod_{i=1}^n P(Y_i = y_i | x_i; \beta) = \prod_{i=1}^n \Phi(y_i x_i^T \beta)
\end{equation*}
The negative log-likelihood that we wish to minimize is:
\begin{equation*}
    \mathcal{L}(\beta) = -\sum_{i=1}^n \log \Phi(y_i x_i^T \beta)
\end{equation*}

\paragraph{The weighted case:}
We introduce sample weights $w_i \in \mathbb{R}_{>0}$
comprising a weight vector $w \in \mathbb{R}_{>0}^n$.
Further, let $g(z) = -\log \Phi(-z)$.
The objective function now becomes:
\begin{equation*}
    f_w(\beta) = \sum_{i=1}^n w_i g(-y_i x_i^T \beta)
\end{equation*}
To make the notation easier, we define $z_i = -y_i x_i^T$ and introduce
the matrix $Z \in \mathbb{R}^{n \times d}$ with row vectors $Z_i = z_i$.
This gives us:
\begin{equation*}
    f_w(\beta) = \sum_{i=1}^n w_i g(z_i \beta)
\end{equation*}

\paragraph{Gradient:}
The gradient of the objective function is needed during optimization.
To derive it, we first need the derivative of $g(z)$:
\begin{equation*}
    g'(z) = \frac{d}{dz} - \log \Phi(-z) = \frac{\phi(z)}{\Phi(-z)}
\end{equation*}
Now we can calculate the gradient of the objective function as follows:
\begin{equation*}
    \frac{\partial f_w(\beta)}{\partial \beta} =
    \sum_{i=1}^n w_i \frac{\partial g(z_i \beta)}{\partial \beta} =
    \sum_{i=1}^n w_i z_i g'(z_i \beta)
\end{equation*}

\begin{lemma}
    Let $g(z) = -\log \Phi(-z)$. Then it holds for all $z \geq 0$ that:
    $$
        \frac{1}{2} z^2 \leq g(z)
    $$
\end{lemma}
\begin{proof}
    The following relationship holds for all $z \geq 1$:
    \begin{align*}
        \Phi(-z) & = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{-z} \exp{ \left(-\frac{1}{2} x^2 \right)} dx       \\
                 & \leq \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{-z} -x \exp{ \left(-\frac{1}{2} x^2 \right)} dx \\
                 & = \frac{1}{\sqrt{2 \pi}} \exp{\left( -\frac{1}{2} z^2 \right)}                              \\
                 & \leq \exp{\left( -\frac{1}{2} z^2 \right)}                                                  \\
    \end{align*}
    We therefore have for $z \geq 1$:
    $$
        e^{g(z)} = e^{-\log \Phi(-z)} = \frac{1}{\Phi(-z)} \geq e^{\frac{1}{2} z^2}
    $$
    Since $\exp( \cdot )$ is a monotonically increasing function,
    it follows that $g(z) \geq \frac{1}{2}z^2$ for all $z \geq 1$.

    \noindent{}Let us now turn to the case when $0 \leq z \leq 1$.
    Both $g(z)$ and $\frac{1}{2}z^2$ are monotonically increasing
    and continuous functions for $0 \leq z \leq 1$.
    Together with the fact that $g(0) > \frac{1}{2}$ it follows
    for all $0 \leq z \leq 1$ that
    $$
        g(z) \geq g(0) > \frac{1}{2} = \max_{0 \leq z \leq 1} \frac{1}{2} z^2 \geq \frac{1}{2} z^2
    $$
    which concludes the proof.
\end{proof}

\begin{lemma}
    Let $g(z) = -\log \Phi(-z)$. Then it holds for all $z \geq 2$ that:
    $$
        g(z) \leq z^2
    $$
\end{lemma}
\begin{proof}
    We first show that $\Phi(-z) \geq \frac{1}{\sqrt{2 \pi}} \frac{z}{z^2 + 1} e^{-\frac{1}{2} z^2}$
    for all $z \geq 0$.
    In order to prove this lower bound, we define
    $h(z) = \Phi(-z) - \frac{1}{\sqrt{2 \pi}} \frac{z}{z^2 + 1} e^{-\frac{1}{2} z^2}$
    and show that $h(z)$ is positive for all $z \geq 0$.
    The derivative $h'(z) = -\sqrt{\frac{2}{\pi}} \frac{e^{-\frac{1}{2} z^2}}{(z^2 + 1)^2}$ is
    negative for all $z$, so $h(z)$ is a monotonically decreasing function.
    Also, it clearly holds that $h(0) > 0$ and
    $\lim_{z \rightarrow \infty} h(z) = 0$. It follows that $h(z) \geq 0$
    for all $z > 0$ which proves the lower bound.

    In the next step, we use this result to show that $e^{z^2} \cdot \Phi(-z) \geq 1$
    for all $z \geq 2$:
    \begin{align*}
        e^{z^2} \cdot \Phi(-z) & \geq e^{z^2} \frac{1}{\sqrt{2 \pi}} \frac{z}{z^2 + 1} e^{-\frac{1}{2} z^2}                           \\
                               & = e^{\frac{1}{2} z^2} \frac{1}{\sqrt{2 \pi}} \frac{z}{z^2 + 1}                                       \\
                               & = e^{\frac{1}{2} z^2} \frac{1}{\frac{4}{3}\left( z^2 + 1 \right)} \frac{\frac{4}{3} z}{\sqrt{2 \pi}} \\
                               & \geq \frac{e^{\frac{1}{2} z^2}}{\frac{4}{3}\left( z^2 + 1 \right)}                                   \\
                               & \geq \frac{e^{\frac{1}{2} z^2}}{e^{\frac{1}{2} z^2}}                                                 \\
                               & = 1
    \end{align*}
    From this it follows directly that $\frac{1}{\Phi(-z)} \leq e^{z^2}$
    and thus we have for all $z \geq 2$:
    $$
        e^{g(z)} = e^{-\log \Phi(-z)} = \frac{1}{\Phi(-z)} \leq e^{z^2}
    $$
    Since $\exp{\left( \cdot \right)}$ is monotonically increasing, the claim
    that $g(z) \leq z^2$ for all $z \geq 2$ follows as a direct consequence.

    The ideas for these proofs are based on the work in~\cite{gaussian_bounds}.
\end{proof}