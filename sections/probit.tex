\section{The Probit Model}

Suppose we are given a dataset
$\mathcal{D} = \left\{ (x_i, y_i) \right\}_{i=1}^n$
containing $n$ pairs of observations.
We assume that the $x_i$ are $d$-dimensional vectors, i.e.
$x_i \in \mathbb{R}^d$ that contain explanatory information regarding
the binary outcome $y_i \in \{ 0,  1 \}$.
Perhaps the $x_i$ are used to represent information about a patient,
such as blood pressure or weight, and the $y_i$ are used to indicate
the presence or absence of a heart disease.
In such a setting, we are often interested in modeling the relationship
between explanatory values of $x_i$ and the binary outcomes $y_i$.

For convenience, we put all the observations $x_i$ inside of a
matrix $X \in \mathbb{R}^{n \times d}$ in such a way that
the $i$-th row of $X$ corresponds to $x_i$.
We do the same with the values of $y_i$ and put them in a
vector $y \in \{0, 1\}^n$.

Since it is reasonable to assume that there is a degree of randomness
involved in the data generating process, we model the $y_i$ as realizations
of independent random variables $Y_i$ that can be summarized
as a single random vector $Y$, where $Y_i$ is the $i$-th component
of $Y$. This brings us to the first assumption in the probit model:
We assume that the observations are independent, i.e. their outcomes don't
influence each other.

The second assumption of the probit model is that there is
a hidden random quantity
$Y_i^\ast$ that is associated with each outcome $Y_i$ in that it
directly determines its result like this:
\begin{equation}
    Y_i =
    \begin{cases}
        1, & \text{if}\ Y_i^\ast > 0    \\
        0, & \text{if}\ Y_i^\ast \leq 0
    \end{cases}
\end{equation}
These $Y_i^\ast$, that can also be summarized as a random vector $Y^\ast$,
are also assumed to be independent and, as already noted, unobservable.
The third assumption of the probit model is, that the observed values
$x_i$ influence $Y_i^\ast$ in the form of a classical linear model:
\begin{equation}
    Y^\ast = X \beta + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2 I),
\end{equation}
where $\beta \in \mathbb{R}^d$ is the parameter vector of the linear model,
$\epsilon$ is a normal distributed vector with independent components of
mean zero and variance $\sigma^2$,
and
$I \in \mathbb{R}^{n \times n}$ is the $n \times n$ identity matrix.
It follows directly that $Y^\ast$ is also normal distributed:
$Y^\ast \sim \mathcal{N}(X \beta, \sigma^2 I)$.

These three assumptions are already a complete specification of the
probit model and are summarized in the following definition as a
brief recapitulation:

\begin{definition}[Probit Model]
    A dataset $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$ with model matrix
    $X \in \mathbb{R}^{n \times d}$ and observed response vector
    $y \in \{0, 1\}^n$ was generated by a probit model with
    parameters $\beta \in \mathbb{R}^d$ and $\sigma \in \mathbb{R}_{>0}$, if
    the following three assumptions are true:
    \begin{enumerate}
        \item The observations $y_1, ..., y_n$ are realizations of independent
              binary random variables $Y_1, ..., Y_n$.
        \item The outcomes of $Y_1, ..., Y_n$ are determined by hidden
              continuous random variables $Y_1^\ast, ..., Y_n^\ast$ by
              thresholding: If $Y_i^\ast > 0$, then $Y_i = 1$, and if
              $Y_i^\ast \leq 0$, then $Y_i = 0$.
        \item The vector of hidden variables $Y^\ast$ follows a multivariate
              normal distribution:
              $Y^\ast \sim \mathcal{N}(X \beta, \sigma^2 I)$,
              where $\beta \in \mathbb{R}^d$ and $\sigma \in \mathbb{R}_{>0}$
              are the model parameters.
    \end{enumerate}
\end{definition}

From this definition, it is straight forward to determine the
distribution of the response variables $Y_i$.
We can calculate the probability $P(Y_i = 1)$ like this:
\begin{equation*}
    P(Y_i = 1) = P(Y_i^\ast > 0) = 1 - P(Y_i^\ast \leq 0)
    = 1 - P\left(\frac{Y_i^\ast - x_i \beta}{\sigma} \leq -\frac{x_i \beta}{\sigma} \right)
    = \Phi\left(\frac{x_i \beta}{\sigma} \right),
\end{equation*}
where $\Phi(\cdot)$ is the cumulative distribution function of the standard normal
distribution:
\begin{equation*}
    \Phi(x) = \int_{-\infty}^x \frac{1}{\sqrt{2 \pi}} e^{- \frac{1}{2} z^2} dz.
\end{equation*}

The result $P(Y_i = 1) = \Phi\left(\frac{x_i \beta}{\sigma} \right)$
leads us to an interesting observation:
Both parameters $\beta$ and $\sigma$ are unknown model parameters and
every value of $\sigma$ can be compensated by a corresponding scaling
of $\beta$. This means that, because we can't observe the hidden variables $Y_i^\ast$,
it is impossible to determine which $\beta$ and which $\sigma$
generated the data without any prior knowledge.
We can only draw conclusions with regard to the
scaled parameter $\frac{1}{\sigma}\beta$.
In this situation, we say that $\beta$ and $\sigma$ are
\textit{not identifiable}.

For this reason, we can assume without losing generality, that
$\sigma = 1$ and arrive at
\begin{equation}
    P(Y_i = 1) = \Phi(x_i \beta).
\end{equation}

\noindent{}Since $Y_i$ is binary, it follows that
\begin{equation*}
    P(Y_i = 0) = 1 - P(Y_i = 1) = 1 - \Phi(x_i \beta) = \Phi(-x_i \beta),
\end{equation*}
which immediately leads us to the model equations
\begin{equation}
    \label{eq:probit-model}
    Y_i \sim Bin(1, \pi_i), \quad \pi_i = \Phi(x_i \beta).
\end{equation}

\subsection{A Special Case of the Generalized Linear Model}

The final equations of the probit model that we arrived at
in equation~\ref{eq:probit-model} are a special case of a more
general model concept, the generalized linear model (GLM).
We briefly touch on this relationship now, because it implies
that some important results of GLMs regarding parameter
estimation can directly be applied to the probit model as well.

Generalized linear models consist of three components.
The first one is the so called \textbf{random component},
a set of $n \in \mathbb{N}$ independent random variables $\{ Y_i \}_{i=1}^n$.
In GLMs, the distribution of these random variables is assumed
to be a member of the \textit{exponential family}, a broad family of
probability distributions that encompasses the normal distribution,
the binomial distribution and many others.
Details can be found in TODO.

The second component of a GLM is the \textbf{linear predictor}.
Just like in the probit model, we also assume that we are
presented with some fixed observations $\{x_i \in \mathbb{R}^d\}_{i=1}^n$,
that are assumed to have some explanatory power with regard to
the $Y_i$. We thus call these observations the explanatory variables.
The linear predictor will be used to relate the explanatory variables
to the distribution of the $Y_i$ by linearly combining them as follows:
\begin{equation*}
    \eta_i = x_i \beta,
\end{equation*}
where $\eta_i \in \mathbb{R}$ denotes the linear predictor related
to observation $x_i$ and
$\beta \in \mathbb{R}^d$ is the unknown parameter vector of the GLM
that has to be estimated when fitting the model.

The third component of a GLM is the so called \textbf{link function}.
This is a monotonic and differentiable function
$g$
that connects the linear predictor $\eta_i$ to the distribution of the
$Y_i$ like this:
\begin{equation*}
    g(E[Y_i]) = \eta_i.
\end{equation*}
We are thus using the link function $g$ to transform the expected value
$E[Y_i]$ in such a way that it can be predicted by a linear model,
hence the name \textit{generalized linear models}.

Equivalently, we can also characterize this relationsthip by using
the inverse function $h = g^{-1}$,
also called the \textbf{response function}:
\begin{equation*}
    E[Y_i] = h(\eta_i).
\end{equation*}
Due to the requirement that $g$ is monotonic and differentiable,
this function $h$ always exists.

We are now ready to establish the connection between the probit model and
the generalized linear model.
As we saw in equation~\ref{eq:probit-model}, the assumptions of the
probit model imply that the $Y_i$ follow independent binomial distributions
with a success probability of $\pi_i = \Phi(x_i \beta)$.
The binomial distribution is a member of the exponential family, so
we can also think of the $Y_i$ as the random component of a GLM.

It also follows directly from the binomial distribution that
$E[Y_i] = \pi_i$, thus we have from the probit model equations that
$\pi_i = E[Y_i] = \Phi(x_i \beta)$, and equivalently
$\Phi^{-1}(E[Y_i]) = x_i \beta$. Thus, we can think of $\Phi$ as the response
function of a GLM and $\Phi^{-1}$ as the link function, which completes
the specification of the probit model as a special case of GLMs.
This enables us to apply all the theoretical results of GLMs to
the probit model as well.

\subsection{Parameter Estimation}

Generalized linear models and therefore the probit model are usually
estimated by using the \textbf{maximum likelihood} method.
This method seeks to maximize the probability that some observed
data $y_1, ..., y_n$, $n \in \mathbb{N}$ was generated under the
assumption of the model and given some parameter vector
$\beta \in \mathbb{R}^d$.
In the probit model, this likelihood is given as
\begin{equation}
    \mathcal{L}(\beta) = P(Y=y ; \beta) = \prod_{i=1}^n P(Y_i=y_i ; \beta),
\end{equation}
because the $Y_i$ are independent. By using a little trick, we can
write $P(Y_i=y_i;\beta)$ like this:
\begin{equation*}
    P(Y_i = y_i ; \beta) = \Phi[(2y_i - 1) x_i \beta],
\end{equation*}
which enables us to arrive at the likelihood
\begin{equation}
    \mathcal{L}(\beta) = \prod_{i=1}^n P(Y_i=y_i ; \beta)
    = \prod_{i=1}^n \Phi[(2y_i - 1) x_i \beta]
    = \prod_{i=1}^n \Phi(- z_i \beta).
\end{equation}
Here, we introduced the new vector $z_i = - (2y_i - 1) x_i$ to simplify
the notation.

The maximum likelihood estimate for $\beta$ is then given by
\begin{equation}
    \hat{\beta} = \underset{\beta \in \mathbb{R}^d}{\operatorname{argmax}}\
    \mathcal{L}(\beta),
\end{equation}
and for $n \rightarrow \infty$ it holds that $E[\hat{\beta}] = \beta$.
However, for finite sample sizes, the existence of $\hat{\beta}$
cannot be guaranteed and is dependent on the observed data.
An overview of the conditions for the existence and uniqueness of
$\hat{\beta}$ is given in~\cite{probit-computational}.
In particular, there is one important condition shown
in~\cite{probit-existence}, that is related to the concept of
linear separability which we introduce in the following definition.
\begin{definition}[Linear separability]
    \label{def:linear-separability}
    Let $\mathcal{D}=\{(x_i, y_i)\}_{i=1}^n$ be a dataset with
    $x_i \in \mathbb{R}^d$ and $y_i \in \{0, 1\}$ for all $i \in [n]$.
    Let $S_0 = \{i \in [n]:\ y_i = 0\}$ and $S_0 = \{i \in [n]:\ y_i = 1\}$.
    If there exists a $\beta \in \mathbb{R}^d$ such that
    \begin{equation*}
        \forall i \in S_0:\ x_i \beta \leq 0\quad \text{and}\quad \forall i \in S_1:\ x_i \beta > 0,
    \end{equation*}
    then we call $\mathcal{D}$ linearly separable.
\end{definition}

\noindent Intuitively speaking, a dataset is linearly separable if there
exists a hyperplane that perfectly separates the datapoints labeled
with $1$ from the datapoints labeled with $0$.
This property of a dataset can be used to infer the existence of the
maximum likelihood estimate $\hat{\beta}$ as stated in the following
theorem.

\begin{theorem}[\cite{probit-existence}]
    \label{theorem:probit-existence}
    Let $\mathcal{D}=\{(x_i, y_i)\}_{i=1}^n$ be a dataset like in
    definition~\ref{def:linear-separability} and let
    $X \in \mathbb{R}^{n \times d}$ be the corresponding model matrix.
    Under the condition that $rank(X)=d$, i.e. $X$ has full column-rank,
    the maximum likelihood estimate $\hat{\beta}$ for the
    parameter $\beta$ of the probit model
    exists if and only if $\mathcal{D}$ is not linearly separable.
\end{theorem}

\noindent As stated in theorem~\ref{theorem:probit-existence},
the maximum likelihood estimate always exists if the dataset
is not linearly separable, provided the model matrix $X$ has
full column-rank.

\subsubsection{Finding the Maximum Likelihood Estimate}

For the reason that the likelihood function $\mathcal{L}(\beta)$ is
numerically inconvenient to maximize, the natural logarithm is often
applied as a transformation to simplify the optimization problem:
\begin{equation}
    \ell(\beta) = \ln \mathcal{L}(\beta) = \sum_{i=1}^n \ln \Phi(- z_i \beta).
\end{equation}

\noindent Since we later wish to interpret $\ell$ as a loss function, we prefer
to minimize the negative value of $\ell$ rather than maximizing:
\begin{equation}
    f(\beta) = -\ell(\beta)
    = \sum_{i=1}^n \ln \left( \frac{1}{1 - \Phi(z_i \beta)} \right)
    = \sum_{i=1}^n g(z_i \beta).
\end{equation}
Here, we define $g(x) = \ln \left(\frac{1}{1 - \Phi(x)}\right)$
and call it the \textbf{probit loss}, i.e. the loss-function that
determines how much each $z_i$ contributes to the total loss $f(\beta)$
for a given value of $\beta$.
We call $f(\beta)$ the objective function of the probit model.

The optimization of $f$ is usually done by applying the
Newton-Raphson algorithm, an iterative procedure that
starts at some initial guess $\beta^{(0)}$ and successively
updates it like this:
\begin{equation}
    \beta^{(t)} = \beta^{(t-1)} - \left(\frac{\partial^2f(\beta^{(t-1)})}{\partial\beta\partial\beta^T}\right)^{-1}
    \cdot \frac{\partial f(\beta^{(t-1)})}{\partial\beta},
\end{equation}
where $\left(\frac{\partial^2f(\beta^{(t-1)})}{\partial\beta\partial\beta^T}\right)^{-1}$
refers to the inverse of the hessian matrix of $f$, evaluated at
$\beta^{(t-1)}$. Likewise, $\frac{\partial f(\beta^{(t-1)})}{\partial\beta}$
refers to the gradient of $f$, evaluated at $\beta^{(t-1)}$.
The idea behind this procedure is, broadly speaking, to approximate
$f$ locally around $\beta^{(t)}$ as a second degree taylor-polynomial and then
analytically find the minimum of this polynomial. The minimum of this
local polynomial approximation of $f$ is then
iteratively used as the basis for the next step of the
Newton-Raphson algorithm.

It remains to find the gradient as well as the hessian matrix of $f$.
Because $f$ is a sum of the function $g$ evaluated at different points,
it makes sense to first determine the derivative of $g$.
This can be accomplished by using the chain rule as follows:
\begin{equation}
    \begin{split}
        \frac{d}{dx}g(x)
        & = \frac{d}{dx} \ln \left(\frac{1}{1 - \Phi(x)}\right)                 \\
        & = (1 - \Phi(x)) \cdot \frac{d}{dx} \left(\frac{1}{1 - \Phi(x)}\right) \\
        & = (1 - \Phi(x)) \cdot \frac{(-1)}{(1 - \Phi(x))^2} \cdot \frac{d}{dx} (1 - \Phi(x)) \\
        & = \frac{(-1)}{1 - \Phi(x)} \cdot (-1) \cdot \phi(x) \\
        & = \frac{\phi(x)}{1 - \Phi(x)},
    \end{split}
\end{equation}
where $\phi(x)$ is the density function of the standard normal distribution function:
\begin{equation*}
    \phi(x) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2} x^2}.
\end{equation*}

\noindent We can use this result to calculate the gradient of $f$:
\begin{equation}
    \begin{split}
        \frac{\partial}{\partial \beta} f(\beta)
        & = \frac{\partial}{\partial \beta} \sum_{i=1}^n g(z_i \beta) \\
        & = \sum_{i=1}^n z_i \cdot \frac{\partial}{\partial \beta} g(z_i \beta) \\
        & = \sum_{i=1}^n z_i \cdot \frac{\phi(z_i\beta)}{1 - \Phi(z_i \beta)}
    \end{split}
\end{equation}

Next, we need to determine the hessian matrix of $f$. In order to do this,
we again start by finding the second derivative of $g$, this time
using the quotient rule:
\begin{equation}
    \begin{split}
        \frac{d^2}{dx^2}g(x)
        & = \frac{d}{dx} \frac{\phi(x)}{1 - \Phi(x)} \\
        & = \frac{\phi'(x)(1 - \Phi(x)) - \phi(x) \cdot (-1) \cdot \phi(x)}
        {(1 - \Phi(x))^2} \\
        & = \frac{(-1) \cdot x \cdot \phi(x)(1 - \Phi(x)) - \phi(x) \cdot (-1) \cdot \phi(x)}
        {(1 - \Phi(x))^2} \\
        & = \frac{[\phi(x)]^2 - x \cdot \phi(x) \cdot (1 - \Phi(x))}{(1 - \Phi(x))^2} \\
        & = \left(\frac{\phi(x)}{1 - \Phi(x)}\right)^2 - x \cdot \frac{\phi(x)}{1 - \Phi(x)} \\
        & = \frac{\phi(x)}{1 - \Phi(x)} \left( \frac{\phi(x)}{1 - \Phi(x)} - x \right)  \\
        & = g'(x) \cdot (g'(x) - x)
    \end{split}
\end{equation}
We can now use this result to find the hessian matrix of $f$:
\begin{equation}
    \begin{split}
        \frac{\partial^2}{\partial \beta \partial \beta^T} f(\beta)
        & = \sum_{i=1}^n
        \frac{\partial^2}{\partial \beta \partial \beta^T} g(z_i \beta)\\
        & = \sum_{i=1}^n z_i z_i^T g'(z_i \beta)(g'(z_i \beta) - z_i \beta)\\
        & = \sum_{i=1}^n z_i z_i^T
        \frac{\phi(z_i \beta)}{1 - \Phi(z_i \beta)} \left( \frac{\phi(z_i \beta)}{1 - \Phi(z_i \beta)} - z_i \beta \right)  \\
    \end{split}
\end{equation}



\newpage