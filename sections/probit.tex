\section{The Probit Model}

Suppose we are given a dataset
$\mathcal{D} = \left\{ (x_i, y_i) \right\}_{i=1}^n$
containing $n$ pairs of observations.
We assume that the $x_i$ are $d$-dimensional vectors, i.e.
$x_i \in \mathbb{R}^d$ that contain explanatory information regarding
the binary outcome $y_i \in \{ 0,  1 \}$.
Perhaps the $x_i$ are used to represent information about a patient,
such as blood pressure or weight, and the $y_i$ are used to indicate
the presence or absence of a heart disease.
In such a setting we are often interested in modeling the relationship
between explanatory values of $x_i$ and the binary outcomes $y_i$.

For convenience, we put all the observations $x_i$ inside of a
matrix $X \in \mathbb{R}^{n \times d}$ in such a way that
the $i$-th row of $X$ corresponds to $x_i$.
We do the same with the values of $y_i$ and put them in a
vector $y \in \{0, 1\}^n$.

Since it is reasonable to assume that there is a degree of randomness
involved in the data generating process, we model the $y_i$ as realizations
of independent random variables $Y_i$ that can be summarized
as a single random vector $Y$, where $Y_i$ is the $i$-th component
of $Y$. We will use this upper case notation
in the following to distinguish between random variables and their
realizations. This brings us to the first assumption in the probit model:
We assume that the observations are independent, i.e. their outcomes don't
influence each other.

The second assumption of the probit model is that there is
a hidden random quantity
$Y_i^\ast$ that is associated with each outcome $Y_i$ in that it
directly determines its result like this:
\begin{equation}
    Y_i =
    \begin{cases}
        1, & \text{if}\ Y_i^\ast > 0    \\
        0, & \text{if}\ Y_i^\ast \leq 0
    \end{cases}
\end{equation}
These $Y_i^\ast$, that can also be summarized as a random vector $Y^\ast$,
are also assumed to be independent and, as already noted, unobservable.
The third assumption of the probit model is, that the observed values
$x_i$ influence $Y_i^\ast$ in the form of a classical linear model:
\begin{equation}
    Y^\ast = X \beta + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2 I),
\end{equation}
where $\beta \in \mathbb{R}^d$ is the parameter vector of the linear model,
$\epsilon$ is a normal distributed vector with independent components of
mean zero and variance $\sigma^2$,
and
$I \in \mathbb{R}^{n \times n}$ is the $n \times n$ identity matrix.
It follows directly that $Y^\ast$ is also normal distributed:
$Y^\ast \sim \mathcal{N}(X \beta, \sigma^2 I)$.

These three assumptions are already a complete specification of the
probit model and are summarized in the following definition as a
brief recapitulation:

\begin{definition}[Probit Model]
    A dataset $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$ with model matrix
    $X \in \mathbb{R}^{n \times d}$ and observed response vector
    $y \in \{0, 1\}^n$ was generated by a probit model if
    the following three assumptions are true:
    \begin{enumerate}
        \item The observations $y_1, ..., y_n$ are realizations of independent
              binary random variables $Y_1, ..., Y_n$.
        \item The outcomes of $Y_1, ..., Y_n$ are determined by hidden
              continuous random variables $Y_1^\ast, ..., Y_n^\ast$ by
              thresholding: If $Y_i^\ast > 0$, then $Y_i = 1$, and if
              $Y_i^\ast \leq 0$, then $Y_i = 0$.
        \item The vector of hidden variables $Y^\ast$ follows a multivariate
              normal distribution:
              $Y^\ast \sim \mathcal{N}(X \beta, \sigma^2 I)$,
              where $\beta \in \mathbb{R}^d$ is the model parameter.
    \end{enumerate}
\end{definition}

From this definition, it is straight forward to determine the
distribution of the response variables $Y_i$.
We can calculate the probability $P(Y_i = 1)$ like this:
\begin{equation*}
    P(Y_i = 1) = P(Y_i^\ast > 0) = 1 - P(Y_i^\ast \leq 0)
    = 1 - P\left(\frac{Y_i^\ast - x_i \beta}{\sigma} \leq -\frac{x_i \beta}{\sigma} \right)
    = \Phi\left(\frac{x_i \beta}{\sigma} \right),
\end{equation*}
where $\Phi(\cdot)$ is the cumulative distribution function of the standard normal
distribution:
\begin{equation*}
    \Phi(x) = \int_{-\infty}^x \frac{1}{\sqrt{2 \pi}} e^{- \frac{1}{2} z^2} dz.
\end{equation*}

The result $P(Y_i = 1) = \Phi\left(\frac{x_i \beta}{\sigma} \right)$
leads us to an interesting observation:
Both parameters $\beta$ and $\sigma$ are unknown model parameters and
every value of $\sigma$ can be compensated by a corresponding scaling
of $\beta$. This means that, because we can't observe the hidden variables $Y_i^\ast$,
it is impossible to determine which $\beta$ and which $\sigma$
generated the data without any prior knowledge.
We can only draw conclusions with regard to the
scaled parameter $\frac{1}{\sigma}\beta$.
In this situation, we say that $\beta$ and $\sigma$ are
\textit{not identifiable}.

For this reason, we can assume without losing generality, that
$\sigma = 1$ and arrive at
\begin{equation}
    P(Y_i = 1) = \Phi(x_i \beta).
\end{equation}

\noindent{}Since $Y_i$ is binary, it follows that
\begin{equation*}
    P(Y_i = 0) = 1 - P(Y_i = 1) = 1 - \Phi(x_i \beta) = \Phi(-x_i \beta),
\end{equation*}
which immediately leads us to the model equations
\begin{equation}
    Y_i \sim Bin(1, \pi_i), \quad \pi_i = \Phi(x_i \beta).
\end{equation}

\newpage