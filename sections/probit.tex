\section{The Probit Model}

Suppose we are given a dataset
$\mathcal{D} = \left\{ (x_i, y_i) \right\}_{i=1}^n$
containing $n$ pairs of observations.
We assume that the $x_i$ are $d$-dimensional vectors, i.e.
$x_i \in \mathbb{R}^d$ that contain explanatory information regarding
the binary outcome $y_i \in \{ 0,  1 \}$.
Perhaps the $x_i$ are used to represent information about a patient,
such as blood pressure or weight, and the $y_i$ are used to indicate
the presence or absence of a heart disease.
In such a setting we are often interested in modeling the relationship
between explanatory values of $x_i$ and the binary outcomes $y_i$.

For convenience, we put all the observations $x_i$ inside of a
matrix $X \in \mathbb{R}^{n \times d}$ in such a way that
the $i$-th row of $X$ corresponds to $x_i$.
We do the same with the values of $y_i$ and put them in a
vector $y \in \{0, 1\}^n$.

Since it is reasonable to assume that there is a degree of randomness
involved in the data generating process, we model the $y_i$ as realizations
of independent random variables $Y_i$ that can be summarized
as a single random vector $Y$, where $Y_i$ is the $i$-th component
of $Y$. We will use this upper case notation
in the following to distinguish between random variables and their
realizations. This brings us to the first assumption in the probit model:
We assume that the observations are independent, i.e. their outcomes don't
influence each other.

The second assumption of the probit model is that there is
a hidden random quantity
$Y_i^\ast$ that is associated with each outcome $Y_i$ in that it
directly determines its result like this:
\begin{equation}
    Y_i =
    \begin{cases}
        1, & \text{if}\ Y_i^\ast > 0    \\
        0, & \text{if}\ Y_i^\ast \leq 0
    \end{cases}
\end{equation}
These $Y_i^\ast$, that can also be summarized as a random vector $Y^\ast$,
are also assumed to be independent and, as already noted, unobservable.
The third assumption of the probit model is, that the observed values
$x_i$ influence $Y_i^\ast$ in the form of a classical linear model:
\begin{equation}
    Y^\ast = X \beta + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2 I),
\end{equation}
where $\beta \in \mathbb{R}^d$ is the parameter vector of the linear model,
$\epsilon$ is a normal distributed vector with independent components of
mean zero and variance $\sigma^2$,
and
$I \in \mathbb{R}^{n \times n}$ is the $n \times n$ identity matrix.
It follows directly that $Y^\ast$ is also normal distributed:
$Y^\ast \sim \mathcal{N}(X \beta, \sigma^2 I)$.

These three assumptions are already a complete specification of the
probit model and are summarized in the following definition as a
brief recapitulation:

\begin{definition}[Probit Model]
    A dataset $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$ with model matrix
    $X \in \mathbb{R}^{n \times d}$ and observed response vector
    $y \in \{0, 1\}^n$ was generated by a probit model with
    parameters $\beta \in \mathbb{R}^d$ and $\sigma \in \mathbb{R}_{>0}$, if
    the following three assumptions are true:
    \begin{enumerate}
        \item The observations $y_1, ..., y_n$ are realizations of independent
              binary random variables $Y_1, ..., Y_n$.
        \item The outcomes of $Y_1, ..., Y_n$ are determined by hidden
              continuous random variables $Y_1^\ast, ..., Y_n^\ast$ by
              thresholding: If $Y_i^\ast > 0$, then $Y_i = 1$, and if
              $Y_i^\ast \leq 0$, then $Y_i = 0$.
        \item The vector of hidden variables $Y^\ast$ follows a multivariate
              normal distribution:
              $Y^\ast \sim \mathcal{N}(X \beta, \sigma^2 I)$,
              where $\beta \in \mathbb{R}^d$ and $\sigma \in \mathbb{R}_{>0}$
              are the model parameters.
    \end{enumerate}
\end{definition}

From this definition, it is straight forward to determine the
distribution of the response variables $Y_i$.
We can calculate the probability $P(Y_i = 1)$ like this:
\begin{equation*}
    P(Y_i = 1) = P(Y_i^\ast > 0) = 1 - P(Y_i^\ast \leq 0)
    = 1 - P\left(\frac{Y_i^\ast - x_i \beta}{\sigma} \leq -\frac{x_i \beta}{\sigma} \right)
    = \Phi\left(\frac{x_i \beta}{\sigma} \right),
\end{equation*}
where $\Phi(\cdot)$ is the cumulative distribution function of the standard normal
distribution:
\begin{equation*}
    \Phi(x) = \int_{-\infty}^x \frac{1}{\sqrt{2 \pi}} e^{- \frac{1}{2} z^2} dz.
\end{equation*}

The result $P(Y_i = 1) = \Phi\left(\frac{x_i \beta}{\sigma} \right)$
leads us to an interesting observation:
Both parameters $\beta$ and $\sigma$ are unknown model parameters and
every value of $\sigma$ can be compensated by a corresponding scaling
of $\beta$. This means that, because we can't observe the hidden variables $Y_i^\ast$,
it is impossible to determine which $\beta$ and which $\sigma$
generated the data without any prior knowledge.
We can only draw conclusions with regard to the
scaled parameter $\frac{1}{\sigma}\beta$.
In this situation, we say that $\beta$ and $\sigma$ are
\textit{not identifiable}.

For this reason, we can assume without losing generality, that
$\sigma = 1$ and arrive at
\begin{equation}
    P(Y_i = 1) = \Phi(x_i \beta).
\end{equation}

\noindent{}Since $Y_i$ is binary, it follows that
\begin{equation*}
    P(Y_i = 0) = 1 - P(Y_i = 1) = 1 - \Phi(x_i \beta) = \Phi(-x_i \beta),
\end{equation*}
which immediately leads us to the model equations
\begin{equation}
    \label{eq:probit-model}
    Y_i \sim Bin(1, \pi_i), \quad \pi_i = \Phi(x_i \beta).
\end{equation}

\subsection{A Special Case of the Generalized Linear Model}

The final equations of the probit model that we arrived at
in equation~\ref{eq:probit-model} are a special case of a more
general model concept, the generalized linear model (GLM).
We briefly touch on this relationship now, because it implies
that some important results of GLMs regarding parameter
estimation can directly be applied to the probit model as well.

Generalized linear models consist of three components.
The first one is the so called \textbf{random component},
a set of $n \in \mathbb{N}$ independent random variables $\{ Y_i \}_{i=1}^n$.
In GLMs, the distribution of these random variables is assumed
to be a member of the \textit{exponential family}, a broad family of
probability distributions that encompasses the normal distribution,
the binomial distribution and many others.
Details can be found in TODO.

The second component of a GLM is the \textbf{linear predictor}.
Just like in the probit model, we also assume that we are
presented with some fixed observations $\{x_i \in \mathbb{R}^d\}_{i=1}^n$,
that are assumed to have some explanatory power with regard to
the $Y_i$. We thus call these observations the explanatory variables.
The linear predictor will be used to relate the explanatory variables
to the distribution of the $Y_i$ by linearly combining them as follows:
\begin{equation*}
    \eta_i = x_i \beta,
\end{equation*}
where $\eta_i \in \mathbb{R}$ denotes the linear predictor related
to observation $x_i$ and
$\beta \in \mathbb{R}^d$ is the unknown parameter vector of the GLM
that has to be estimated when fitting the model.

The third component of a GLM is the so called \textbf{link function}.
This is a monotonic and differentiable function
$g$
that connects the linear predictor $\eta_i$ to the distribution of the
$Y_i$ like this:
\begin{equation*}
    g(E[Y_i]) = \eta_i.
\end{equation*}
We are thus using the link function $g$ to transform the expected value
$E[Y_i]$ in such a way that it can be predicted by a linear model,
hence the name \textit{generalized linear models}.

Equivalently, we can also characterize this relationsthip by using
the inverse function $h = g^{-1}$,
also called the \textbf{response function}:
\begin{equation*}
    E[Y_i] = h(\eta_i).
\end{equation*}
Due to the requirement that $g$ is monotonic and differentiable,
this function $h$ always exists.

We are now ready to establish the connection between the probit model and
the generalized linear model.
As we saw in equation~\ref{eq:probit-model}, the assumptions of the
probit model imply that the $Y_i$ follow independent binomial distributions
with a success probability of $\pi_i = x_i \beta$.
The binomial distribution is a member of the exponential family, so
we can also think of the $Y_i$ as the random component of a GLM.

It also follows directly from the binomial distribution that
$E[Y_i] = \pi_i$, thus we have from the probit model equations that
$\pi_i = E[Y_i] = \Phi(x_i \beta)$, and equivalently
$\Phi^{-1}(E[Y_i]) = x_i \beta$. Thus, we can think of $\Phi$ as the response
function of a GLM and $\Phi^{-1}$ as the link function, which completes
the specification of the probit model as a special case of GLMs.
This enables us to apply all the theoretical results of GLMs to
the probit model as well.



\newpage