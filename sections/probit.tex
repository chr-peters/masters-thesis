\section{The Probit Model}

Suppose we are given a dataset
$\mathcal{D} = \left\{ (x_i, y_i) \right\}_{i=1}^n$
containing $n$ pairs of observations.
We assume that the $x_i$ are $d$-dimensional vectors, i.e.
$x_i \in \mathbb{R}^d$ that contain explanatory information regarding
the binary outcome $y_i \in \{ 0,  1 \}$.
Perhaps the $x_i$ are used to represent information about a patient,
such as blood pressure or weight, and the $y_i$ are used to indicate
the presence or absence of a heart disease.
In such a setting we are often interested in modeling the relationship
between explanatory values of $x_i$ and the binary outcomes $y_i$.

For convenience, we put all the observations $x_i$ inside of a
matrix $X \in \mathbb{R}^{n \times d}$ in such a way that
the $i$-th row of $X$ corresponds to $x_i$.
We do the same with the values of $y_i$ and put them in a
vector $y \in \{0, 1\}^n$.

Since it is reasonable to assume that there is a degree of randomness
involved in the data generating process, we model the $y_i$ as realizations
of independent random variables $Y_i$ that can be summarized
as a single random vector $Y$, where $Y_i$ is the $i$-th component
of $Y$. We will use this upper case notation
in the following to distinguish between random variables and their
realizations. This brings us to the first assumption in the probit model:
We assume that the observations are independent, i.e. their outcomes don't
influence each other.

The second assumption of the probit model is that there is
a hidden random quantity
$Y_i^\ast$ that is associated with each outcome $Y_i$ in that it
directly determines its result like this:
\begin{equation}
    Y_i =
    \begin{cases}
        1, & \text{if}\ Y_i^\ast \geq 0 \\
        0, & \text{if}\ Y_i^\ast < 0
    \end{cases}
\end{equation}
These $Y_i^\ast$, that can also be summarized as a random vector $Y^\ast$,
are also assumed to be independent and, as already noted, unobservable.
The third assumption of the probit model is, that the observed values
$x_i$ influence $Y_i^\ast$ in the form of a classical linear model:
\begin{equation}
    Y^\ast = X \beta + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2 I),
\end{equation}
where $\beta \in \mathbb{R}^d$ is the parameter vector of the linear model,
$\epsilon$ is a normal distributed vector with independent components of
mean zero and variance $\sigma^2$,
and
$I \in \mathbb{R}^{n \times n}$ is the $n \times n$ identity matrix.
It follows directly that $Y^\ast$ is also normal distributed:
$Y^\ast \sim \mathcal{N}(X \beta, \sigma^2 I)$.

\newpage