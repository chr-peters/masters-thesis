\section{Introduction}

When presented with the task of analyzing a dataset, the modern
data scientist has a plethora of different methods and algorithms
at their dispoal, ranging from simple linear models all the way up to
sophisticated neural networks.
In most cases, the actual implementation of said algorithms is only
a minor concern for the data scientist, as there is an abundance of
ready to use software packages available, where even the most
advanced models can be defined in only a few lines of code.
As long as the data fits nicely into the main memory of the
data scientist's machine, everything is well and most algorithms
perform as expected, delivering their solutions within a reasonable
amount of time.

However, there is an increasing number of situations, where
reality gets more and more complicated, especially when the
datasets drastically outgrow the storage capabilities
of the data scientist's machine.
The authors of \cite{big-data-tiny-data} present a list of such
scenarios, including for example the vast amounts of data
delivered by the sensors of mobile devices, cameras,
internet logs or the financial markets.
In such situations, the data scientist is faced with a problem:
The tried and tested algorithms of his arsenal become inefficient.
There is simply too much data!

Now, the question arises if all is lost and we have to reinvent
every existing algorithm from scratch to deal with the scalability
problems that come along with big data.
Luckily, a recent line of research has started to emerge from
these questions, showing an alternative way of dealing with
the problem, that doesn't require us to throw away all the established
algorithms of data analysis, which have been so carefully constructed.

One of the main ideas behind this new research is the
\textit{sketch-and-solve} paradigm
(see for example \cite{woodruff-2014}):
Instead of changing and adapting each of our existing algorithms,
we find ways to reduce the data first,
so that in a second step, the existing algorithms can be
applied to the resulting much smaller datasets more efficiently.
The advantages of this approach are clear: We only need to
focus on the data reduction problem and don't need to change
any of the existing algorithms. However, this also brings
with it new challenges: How can such algorithms for data
reduction be designed? And perhaps most importantly:
Are there any provable guarantees that can be formulated
regarding the outcome of sketch-and-solve?
After all, we obviously want that the algorithms applied to
the reduced data deliver approximately the same results as
if the algorithms were applied to the original data.

In this work, we tackle the challenge of finding efficient
data reduction algorithms with provable guarantees
in the domain of probit regression, one of the most important
linear models for binary response data with broad areas of
application, including biostatistics \cite{probit-biostatistics}
and econometrics \cite{probit-econometrics}.
One advantage of the probit model over similar models such
as logistic regression, which makes the probit model particularly
interesting for the use in Bayesian data analysis, is that
there exists an efficient Gibbs sampling algorithm
\cite{gibbs-probit-albert-chib} for obtaining samples of the
posterior distribution. Gibbs sampling is another notoriously
costly computational procedure on large datasets,
and we will see in this work, how it can also benefit from
applying a data reduction algorithm beforehand in order
to cut down on its computational demands.

In order to move towards constructing our data reduction algorithms,
we follow a similar roadmap as the authors
of \cite{on-coresets}, who derived similar algorithms in the
context of logistic regression as well.
Inspired by their success, we make use of the theory of the
sensitivity framework \cite{big-data-tiny-data}, a theorem
that helps us to design a sampling distribution, according
to which we can sample a small subset of the original data,
also known as a \textit{coreset}
(see for example \cite{munteanu-coresets-introduction}),
that admits strong theoretical
guarantees regarding its representativeness of the original
dataset.

We use these results as a basis to construct two efficient
data reduction algorithms, one of them requiring two passes
over the data and the other one being an online
algorithm and requiring only one pass.
Finally, we demonstrate the practical relevance of our algorithms
by carrying out a variety of experiments
on three real world datasets,
both in the context of maximum likelihood estimation of the probit model
as well as in the realm of Bayesian probit analysis.
