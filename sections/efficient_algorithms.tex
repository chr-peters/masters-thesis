\section{Efficient Coreset Algorithms}

As we saw when constructing the na\"ive algorithm, there are two
main challenges that we have to deal with in order to make the
algorithm more efficient and suitable for large datasets:
First, we have to find a way to efficiently compute the
leverage scores, preferably without
having to perform a full QR decomposition.
Second, after obtaining the sampling probabilities $p_i$, we
need to be able to sample elements from the dataset
with as little computational overhead as possible, ideally
in one pass over the dataset.
We explore methods for dealing with both of these problems in this
chapter.

\subsection{A Fast Two-Pass Algorithm}

Let's assume for a moment that we already have the sensitivity
bounds $s_1, ..., s_n$ available and that we are now
interested in independently sampling $k$ elements from our dataset
$\mathcal{D} = \{(x_1, y_i)\}_{i=1}^n$, such that the
$i$-th element has a probability of
$p_i = \frac{s_i}{\sum_{j=1}^n s_j}$ of being sampled.
Luckily, there already exist multiple different algorithms
that solve exactly this problem in only one pass over the
dataset, i.e. by only looking at each element in $\mathcal{D}$
once. One of these algorithms, that will be used in this
work, is the so-called reservoir sampler by Chao~\cite{reservoir-sampler}.

\subsubsection{One-Pass Sampling with a Reservoir}

As the name suggests, the reservoir sampler consists of a reservoir,
i.e. a storage of size $k$, where the resulting sample will be stored.
In the beginning of the sampling procedure, the reservoir is empty.
Next, the algorithm decides for each element of $\mathcal{D}$
if it is added to the reservoir. In the first $k$ steps of the
procedure, when there is still room in the reservoir, every item is
added and the reservoir is filled. After that, when the reservoir is
full and there are still elements left in $\mathcal{D}$,
the algorithm has to decide for each new element, if it should be added
to the reservoir, and if yes,
which element of the reservoir it should replace.
These two decisions are the main ingredients of the algorithm, but
as shown in \cite{reservoir-sampler}, they turn out to be
relatively simple rules.

In order to decide, if a new sample with bound $s_j$ should be
included in the reservoir, the algorithm maintains at each step
the sum $S_j = \sum_{l=1}^j s_l$. The decision, whether
the new element is included is then based on sampling
a uniformly distributed number $q \sim U(0, 1)$.
If $q \geq \frac{k \cdot s_j}{S_j}$, the new element is included,
otherwise it is ignored. In case the element is included, the
algorithm still has to decide, which element has to be released
from the reservoir. But this decision also turns out to be simple:
It suffices, to just select an element from the reservoir at
random and replace it with the new element.
As shown in \cite{reservoir-sampler}, both of these rules together
ensure, that after one pass over the entire dataset, the
reservoir contains the desired sample.

In order to use this algorithm for our purposes, there is one
little adjustment that we have to make. The reservoir sampler
that we just described samples the elements without replacement,
but one little subtlety of the sensitivity framework is that the
elements are actually sampled with replacement.
It turns out, that this difference can easily be overcome:
Instead of using a single reservoir sampler with a reservoir of
size $k$, we can use $k$ independent reservoir samplers, where
each instance has a reservoir of size 1. Every element of
$\mathcal{D}$ is then fed into all the $k$ instances, and in the
end we obtain our sample from the $k$ reservoirs. This way,
we can simulate sampling with replacement.

Having now found a solution for the problem of efficiently sampling
elements from the dataset in only one pass, we can now turn
our attention to the other problem of efficiently computing
the leverage scores.

\subsubsection{Fast Approximation of Statistical Leverage Scores}

In order to avoid a full QR decomposition, we use a method
described in~\cite{leverage-scores-drineas} and improved
in~\cite{woodruff-2017} to obtain approximations of the
leverage scores in an efficient manner.

The idea behind this procedure is, instead of obtaining the QR
decomposition of $\sqrt{D_w}Z$, which is expensive and needs
$O(nd^2)$ time, we first transform $\sqrt{D_w}Z$ into a
much smaller matrix $\tilde Z \in \mathbb{R}^{t \times d}$
and then obtain the QR decomposition $\tilde{Z} = \tilde Q \tilde R$,
which now only takes $O(td^2)$ time, depending on the
reduced size $t$.
Using the matrix $\tilde{R}$ of the reduced QR decomposition, we
can approximate the leverage scores by computing the squared row norms of
the matrix $\sqrt{D_w}Z \tilde{R}^{-1}$ (although we will later
see, how we can also speed up this step).
The interesting part of this procedure is, how we can obtain
the reduced matrix $\tilde{Z}$, and which criteria $\tilde{Z}$
must satisfy in order for this method to work.

To acquire $\tilde{Z}$, the authors of \cite{woodruff-2017} construct
a so-called \textit{subspace embedding}.
In order to understand, what that means, suppose that we have an
arbitrary matrix $A \in \mathbb{R}^{n \times d}$.
When talking about a subspace embedding, we are referring to a matrix
$S \in \mathbb{R}^{t \times n}$, such that
\begin{equation*}
    (1 - \epsilon) \lVert Ax \rVert_2 \leq \lVert SAx \rVert_2
    \leq (1 + \epsilon) \lVert Ax \rVert_2
\end{equation*}
for every $x \in \mathbb{R}^d$ simultaneously and $\epsilon > 0$.
This equation has a profound meaning: When viewing $A$ as a
collection of
$d$ column-vectors in $\mathbb{R}^n$, each of these vectors gets
mapped into the lower dimensional space $\mathbb{R}^t$, but all the
distances between the original vectors as well as their lengths
are preserved. For example, by choosing
$x = (1, 0, ..., 0)^T \in \mathbb{R}^d$, we can see that the norm
of the first column vector of $A$ is preserved in the
$t$-dimensional subspace up to a factor of $(1 \pm \epsilon)$.
Likewise, by choosing $x = (1, -1, 0, ..., 0)^T \in \mathbb{R}^d$,
we can also see that the distance between the first two column
vectors is preserved in the lower dimensional space as well.
By this logic, we can see that not only are all the lengths
and distances of the original vectors preserved in the
subspace, but also
the lengths and distances between every possible linear
combination of the original column vectors. In this way, the
whole column space of $A$ is embedded into the lower dimensional
subspace.

The question now is how to choose the embedding matrix $S$ in
such a way, that $SA$ can be
computed efficiently and that the reduced size $t$ is as small
as possible. In order to do so, the authors of
\cite{woodruff-2017} developed an efficient procedure,
which not only makes it possible to construct subspace
embedding matrices obliviously from the data at hand, but
also makes it possible to compute the product $SA$
in time $O(\operatorname{nnz}(A))$, i.e. the number of
non-zero entries in $A$. Further, a reduction size of
$t \in O(d^2)$ is already enough to obtain a subspace embedding.
This is independent of the number of data points $n$ and
can be considered a huge advantage.

Surprisingly, the suggested
procedure can easily be described in only two simple steps:
In the first step, an all zero matrix
$\tilde{A} \in \mathbb{R}^{d^2 \times d}$ is initialized, where
the result of the multiplication $SA$ will be stored.
Next, each row of $A$ is first multiplied by $+1$ or $-1$
with equal probability and then randomly added to one of the
rows of $\tilde{A}$, also with equal probability.
It turns out, that this simple procedure already yields the
result of $\tilde{A} = SA$ for an embedding matrix $S$ which
represents the procedure, and that $\tilde{A}$ can be
computed not only in time $O(\operatorname{nnz}(A))$, but also
in a single row by row pass over the matrix $A$.

The first step of the fast leverage score approximation procedure
introduced in \cite{leverage-scores-drineas} is to apply such
an embedding procedure to the matrix, for which the leverage scores
should be approximated, in our case $\sqrt{D_w}Z$, which will
yield a matrix $\tilde{Z} \in \mathbb{R}^{d^2 \times d}$.
This can be done in one row by row pass over the data, because
every row just has to be multiplied randomly with +1 or -1 and
then added to one of the rows in the result matrix $\tilde{Z}$.
In the next step, we perform a QR decomposition
$\tilde{Z} = \tilde{Q} \tilde{R}$, which now takes
$O(d^4)$ time. The resulting
matrix $\tilde{R}$ can then be used to approximate
the leverage scores in a second pass over the data by computing
$\tilde{\ell_i} = \lVert w_i z_i \tilde{R}^{-1} \rVert_2^2$, where $z_i$
is the $i$-th row-vector of $Z$.

The computation of $\tilde{\ell_i}$ takes time $O(d^2)$, but
the authors of \cite{leverage-scores-drineas} suggest another
procedure to speed up this time to $O(d \log(n))$, which can be
a useful step if $d > \log(n)$.
Their idea is to apply a so-called Johnson-Lindenstrauss
transformation \cite{johnson-lindenstrauss} to $R^{-1}$
in order to reduce its size to only $d \times m$ elements,
where $m \in O(\log(n))$. What this means is that the matrix
$R^{-1} \in \mathbb{R}^{d \times d}$ is multiplied by a random matrix
$G \in \mathbb{R}^{d \times m}$, where each entry of $G$
follows a normal distribution with mean zero and variance
$\frac{1}{m}$, i.e. $G_{ij} \sim \mathcal{N}(0, \frac{1}{m})$.
The resulting product $R^{-1}G$ can be computed once in the beginning,
and after that every single row by row pass takes only
$O(d \log(n))$ time to compute the approximated leverage
score $\tilde{\ell}_i = \lVert \sqrt{w_i} z_i (R^{-1} G) \rVert_2^2$.
It was shown in \cite{leverage-scores-drineas}, that the
approximations $\tilde{\ell}_i$ of the leverage scores satisfy that
\begin{equation*}
    (1 - \epsilon) \ell_i \leq \tilde{\ell}_i \leq (1 + \epsilon) \ell_i
\end{equation*}
for $\epsilon > 0$, where $\ell_i$ is the true leverage score.
We thus have obtained a constant factor
approximation of the true leverage scores that can be computed
efficiently in only two passes over the data and we can argue,
that the constant factor approximation of the leverage scores
doesn't affect the asymptotic analysis of the sensitivity
framework (see theorem~\ref{theorem:sensitivity-framework}).
Thus, we can replace the true leverage scores with the approximated
leverage scores in our sampling distribution without having any
impact on the coreset size.

\begin{algorithm}
    \DontPrintSemicolon
    \KwIn{Dataset $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$ with
    scaled model matrix $Z \in \mathbb{R}^{n \times d}$
    and weight vector $w \in \mathbb{R}_{>0}^n$,
    $\mathcal{W} = \sum_{i=1}^n w_i$,
    size parameter $k \in \mathbb{N}$}
    \KwOut{A subset $\mathcal{C} \subseteq \mathcal{D}$
        of size $|\mathcal{C}| = k$ with weight vector
        $u \in \mathbb{R}_{>0}^k$}
    Initialize $\tilde{Z} = 0 \in \mathbb{R}^{d^2 \times d}$ \;
    \For{$i = 1,...,n$}{
        $z_i := i$'th row of $Z$ \;
        $j = $ random sample from $\{1, ..., d^2\}$ with equal probability \;
        $l = $ random sample from $\{+1, -1\}$ with equal probability \;
        $\tilde{Z}_j = \tilde{Z}_j + l \cdot \sqrt{w_i} z_i$ \tcc*[r]{update the $j$'th row of $\tilde{Z}$}
    }
    Compute the QR decomposition of $\tilde{Z} = \tilde{Q}\tilde{R}$ \;
    Initialize $G = I \in \mathbb{R}^{d \times d}$ \;
    \If{$\lceil\log(n)\rceil < d$} {
        $G = 0 \in \mathbb{R}^{d \times \lceil\log(n)\rceil}$ \;
        Draw $G_{ij} \sim \mathcal{N}(0, \frac{1}{\lceil\log{n}\rceil})$
        \tcc*[r]{draw Johnson-Lindenstrauss matrix}
    }
    Compute $M = \tilde{R}^{-1} G$ \;
    Initialize $u = 0 \in \mathbb{R}^k$\tcc*[r]{empty weight vector}
    Initialize $k$ independent weighted size-1-reservoir samplers $S_1, ..., S_k$ \;
    \For{$i = 1, ..., n$}{
        $\tilde{\ell}_i = \lVert \sqrt{w_i} z_i M\rVert_2^2$ \;
        $s_i = \tilde{\ell}_i + \frac{1}{\mathcal{W}}$ \;
        \For{$j = 1, ..., k$} {
            Feed $z_i$ with sampling weight $s_i$ to $S_j$ \;
            \If{$S_j$ \textup{samples} $z_i$}{
                $u_j = \frac{w_j}{s_i k}$
            }
        }
    }
    $\mathcal{C} := $ elements from the $k$ reservoirs \;
    $u = u \cdot \sum_{i=1}^n s_i$\tcc*[r]{normalize weights}
    \KwRet{$\mathcal{C}^{(k)}$, $u$}
    \caption{A fast two-pass algorithm for coreset construction\label{algo:two-pass}}
\end{algorithm}

\subsection{A One-Pass Online Algorithm}

\subsubsection{Online Approximation of Statistical Leverage Scores}
