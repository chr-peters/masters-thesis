\section{Efficient Coreset Algorithms}

As we saw when constructing the na\"ive algorithm, there are two
main challenges that we have to deal with in order to make the
algorithm more efficient and suitable for large datasets:
First, we have to find a way to efficiently compute the
leverage scores, preferably without
having to perform a full QR decomposition.
Second, after obtaining the sampling probabilities $p_i$, we
need to be able to sample elements from the dataset
with as little computational overhead as possible, ideally
in one pass over the dataset.
We explore methods for dealing with both of these problems in this
chapter.

\subsection{A Fast Two-Pass Algorithm}

Let's assume for a moment that we already have the sensitivity
bounds $s_1, ..., s_n$ available and that we are now
interested in independently sampling $k$ elements from our dataset
$\mathcal{D} = \{(x_1, y_i)\}_{i=1}^n$, such that the
$i$-th element has a probability of
$p_i = \frac{s_i}{\sum_{j=1}^n s_j}$ of being sampled.
Luckily, there already exist multiple different algorithms
that solve exactly this problem in only one pass over the
dataset, i.e. by only looking at each element in $\mathcal{D}$
once. One of these algorithms, that will be used in this
work, is the so-called reservoir sampler by Chao~\cite{reservoir-sampler}.

\subsubsection{One-Pass Sampling with a Reservoir}

As the name suggests, the reservoir sampler consists of a reservoir,
i.e. a storage of size $k$, where the resulting sample will be stored.
In the beginning of the sampling procedure, the reservoir is empty.
Next, the algorithm decides for each element of $\mathcal{D}$
if it is added to the reservoir. In the first $k$ steps of the
procedure, when there is still room in the reservoir, every item is
added and the reservoir is filled. After that, when the reservoir is
full and there are still elements left in $\mathcal{D}$,
the algorithm has to decide for each new element, if it should be added
to the reservoir, and if yes,
which element of the reservoir it should replace.
These two decisions are the main ingredients of the algorithm, but
as shown in \cite{reservoir-sampler}, they turn out to be
relatively simple rules.

In order to decide, if a new sample with bound $s_j$ should be
included in the reservoir, the algorithm maintains at each step
the sum $S_j = \sum_{l=1}^j s_l$. The decision, whether
the new element is included is then based on sampling
a uniformly distributed number $q \sim U(0, 1)$.
If $q \geq \frac{k \cdot s_j}{S_j}$, the new element is included,
otherwise it is ignored. In case the element is included, the
algorithm still has to decide, which element has to be released
from the reservoir. But this decision also turns out to be simple:
It suffices, to just select an element from the reservoir at
random and replace it with the new element.
As shown in \cite{reservoir-sampler}, both of these rules together
ensure, that after one pass over the entire dataset, the
reservoir contains the desired sample.

In order to use this algorithm for our purposes, there is one
little adjustment that we have to make. The reservoir sampler
that we just described samples the elements without replacement,
but one little subtlety of the sensitivity framework is that the
elements are actually sampled with replacement.
It turns out, that this difference can easily be overcome:
Instead of using a single reservoir sampler with a reservoir of
size $k$, we can use $k$ independent reservoir samplers, where
each instance has a reservoir of size 1. Every element of
$\mathcal{D}$ is then fed into all the $k$ instances, and in the
end we obtain our sample from the $k$ reservoirs. This way,
we can simulate sampling with replacement.

Having now found a solution for the problem of efficiently sampling
elements from the dataset in only one pass, we can now turn
our attention to the other problem of efficiently computing
the leverage scores.

\subsubsection{Fast Approximation of Statistical Leverage Scores}

In order to avoid a full QR decomposition, we use a method
described in~\cite{leverage-scores-drineas} and improved
in~\cite{woodruff-2017} to obtain approximations of the
leverage scores in an efficient manner.

The idea behind this procedure is, instead of obtaining the QR
decomposition of $\sqrt{D_w}Z$, which is expensive and needs
$O(nd^2)$ time, we first transform $\sqrt{D_w}Z$ into a
much smaller matrix $\tilde Z \in \mathbb{R}^{t \times d}$
and then obtain the QR decomposition $\tilde{Z} = \tilde Q \tilde R$,
which now only takes $O(td^2)$ time, depending on the
reduced size $t$.
Using the matrix $\tilde{R}$ of the reduced QR decomposition, we
can approximate the leverage scores by computing the row norms of
the matrix $\sqrt{D_w}Z \tilde{R}^{-1}$ (although we will later
see, how we can also speed up this step).
The interesting part of this procedure is, how we can obtain
the reduced matrix $\tilde{Z}$, and which criteria $\tilde{Z}$
must satisfy in order for this method to work.

To acquire $\tilde{Z}$, the authors of \cite{woodruff-2017} construct
a so-called \textit{subspace embedding}.
In order to understand, what that means, suppose that we have an
arbitrary matrix $A \in \mathbb{R}^{n \times d}$.
When talking about a subspace embedding, we are referring to a matrix
$S \in \mathbb{R}^{t \times n}$, such that
\begin{equation*}
    (1 - \epsilon) \lVert Ax \rVert_2 \leq \lVert SAx \rVert_2
    \leq (1 + \epsilon) \lVert Ax \rVert_2
\end{equation*}
for every $x \in \mathbb{R}^d$ simultaneously and $\epsilon > 0$.
This equation has a profound meaning: When viewing $A$ as a
collection of
$d$ column-vectors in $\mathbb{R}^n$, each of these vectors gets
mapped into the lower dimensional space $\mathbb{R}^t$, but all the
distances between the original vectors as well as their lengths
are preserved. For example, by choosing
$x = (1, 0, ..., 0)^T \in \mathbb{R}^d$, we can see that the norm
of the first column vector of $A$ is preserved in the
$t$-dimensional subspace up to a factor of $(1 \pm \epsilon)$.
Likewise, by choosing $x = (1, -1, 0, ..., 0)^T \in \mathbb{R}^d$,
we can also see that the distance between the first two column
vectors is preserved in the lower dimensional space as well.
By this logic, we can see that not only are all the lengths
and distances of the original vectors preserved in the
subspace, but also
the lengths and distances between every possible linear
combination of the original column vectors. In this sense,
we can use the subspace embedding to reduce the size of a
matrix while still preserving its structure.

\subsection{A One-Pass Online Algorithm}

\subsubsection{Online Approximation of Statistical Leverage Scores}
