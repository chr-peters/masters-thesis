\section{Efficient Coreset Algorithms}

As we saw when constructing the na\"ive algorithm, there are two
main challenges that we have to deal with in order to make the
algorithm more efficient and suitable for large datasets:
First, we have to find a way to efficiently compute the
leverage scores, preferably without
having to perform a full QR decomposition.
Second, after obtaining the sampling probabilities $p_i$, we
need to be able to sample elements from the dataset
with as little computational overhead as possible, ideally
in one pass over the dataset.
We explore methods for dealing with both of these problems in this
chapter.

\subsection{A Fast Two-Pass Algorithm}

Let's assume for a moment that we already have the sensitivity
bounds $s_1, ..., s_n$ available and that we are now
interested in independently sampling $k$ elements from our dataset
$\mathcal{D} = \{(x_1, y_i)\}_{i=1}^n$, such that the
$i$-th element has a probability of
$p_i = \frac{s_i}{\sum_{j=1}^n s_j}$ of being sampled.
Luckily, there already exist multiple different algorithms
that solve exactly this problem in only one pass over the
dataset, i.e. by only looking at each element in $\mathcal{D}$
once. One of these algorithms, that will be used in this
work, is the so-called reservoir sampler by Chao~\cite{reservoir-sampler}.

\subsubsection{One-Pass Sampling with a Reservoir}

As the name suggests, the reservoir sampler consists of a reservoir,
i.e. a storage of size $k$, where the resulting sample will be stored.
In the beginning of the sampling procedure, the reservoir is empty.
Next, the algorithm decides for each element of $\mathcal{D}$
if it is added to the reservoir. In the first $k$ steps of the
procedure, when there is still room in the reservoir, every item is
added and the reservoir is filled. After that, when the reservoir is
full and there are still elements left in $\mathcal{D}$,
the algorithm has to decide for each new element, if it should be added
to the reservoir, and if yes,
which element of the reservoir it should replace.
These two decisions are the main ingredients of the algorithm, but
as shown in \cite{reservoir-sampler}, they turn out to be
relatively simple rules.

In order to decide, if a new sample with sampling weight $s_j$ should be
included in the reservoir, the algorithm maintains at each step
the sum $S_j = \sum_{l=1}^j s_l$. The decision, whether
the new element is included, is then based on sampling
a uniformly distributed number $q \sim U(0, 1)$.
If $q \geq \frac{k \cdot s_j}{S_j}$, the new element is included,
otherwise it is ignored. In case the element is included, the
algorithm still has to decide, which element has to be released
from the reservoir. But this decision also turns out to be simple:
It suffices, to just select an element from the reservoir at
random and replace it with the new element.
As shown in \cite{reservoir-sampler}, both of these rules together
ensure, that after one pass over the entire dataset, the
reservoir contains the desired sample.

In order to use this algorithm for our purposes, there is one
little adjustment that we have to make. The reservoir sampler
that we just described samples the elements without replacement,
but one little subtlety of the sensitivity framework is that the
elements are actually sampled with replacement.
It turns out, that this difference can easily be overcome:
Instead of using a single reservoir sampler with a reservoir of
size $k$, we can use $k$ independent reservoir samplers, where
each instance has a reservoir of size 1. Every element of
$\mathcal{D}$ is then fed into all the $k$ instances, and in the
end we obtain our sample from the $k$ reservoirs. This way,
we can simulate sampling with replacement.

Having now found a solution to the problem of efficiently sampling
elements from the dataset in only one pass, we can now turn
our attention to the other problem of efficiently computing
the leverage scores.

\subsubsection{Fast Approximation of Statistical Leverage Scores}

In order to avoid a full QR decomposition, we use a method
described in~\cite{leverage-scores-drineas} and improved
in~\cite{woodruff-2017} to obtain approximations of the
leverage scores in an efficient manner.

The idea behind this procedure is, instead of obtaining the QR
decomposition of $\sqrt{D_w}Z$, which is expensive and needs
$O(nd^2)$ time, we first transform $\sqrt{D_w}Z$ into a
much smaller matrix $\tilde Z \in \mathbb{R}^{t \times d}$
and then obtain the QR decomposition $\tilde{Z} = \tilde Q \tilde R$,
which now only takes $O(td^2)$ time, depending on the
reduced size $t$.
Using the matrix $\tilde{R}$ of the reduced QR decomposition, we
can approximate the leverage scores by computing the squared row norms of
the matrix $\sqrt{D_w}Z \tilde{R}^{-1}$ (although we will later
see, how we can also speed up this step).
The interesting part of this procedure is, how we can obtain
the reduced matrix $\tilde{Z}$, and which criteria $\tilde{Z}$
must satisfy in order for this method to work.

To acquire $\tilde{Z}$, the authors of \cite{woodruff-2017} construct
a so-called \textit{subspace embedding}.
In order to understand, what that means, suppose that we have an
arbitrary matrix $A \in \mathbb{R}^{n \times d}$.
When talking about a subspace embedding, we are referring to a matrix
$S \in \mathbb{R}^{t \times n}$, such that
\begin{equation*}
    (1 - \epsilon) \lVert Ax \rVert_2 \leq \lVert SAx \rVert_2
    \leq (1 + \epsilon) \lVert Ax \rVert_2
\end{equation*}
for every $x \in \mathbb{R}^d$ simultaneously and $\epsilon > 0$.
This equation has a profound meaning: When viewing $A$ as a
collection of
$d$ column-vectors in $\mathbb{R}^n$, each of these vectors gets
mapped into the lower dimensional space $\mathbb{R}^t$, but all the
distances between the original vectors as well as their lengths
are preserved. For example, by choosing
$x = (1, 0, ..., 0)^T \in \mathbb{R}^d$, we can see that the norm
of the first column vector of $A$ is preserved in the
$t$-dimensional subspace up to a factor of $(1 \pm \epsilon)$.
Likewise, by choosing $x = (1, -1, 0, ..., 0)^T \in \mathbb{R}^d$,
we can also see that the distance between the first two column
vectors is preserved in the lower dimensional space as well.
By this logic, we can see that not only are all the lengths
and distances of the original vectors preserved in the
subspace, but also
the lengths and distances between every possible linear
combination of the original column vectors. In this way, the
whole column space of $A$ is embedded into the lower dimensional
subspace.

The question now is how to choose the embedding matrix $S$ in
such a way, that $SA$ can be
computed efficiently and that the reduced size $t$ is as small
as possible. In order to do so, the authors of
\cite{woodruff-2017} developed an efficient procedure,
which not only makes it possible to construct subspace
embedding matrices obliviously from the data at hand, but
also makes it possible to compute the product $SA$
in time $O(\operatorname{nnz}(A))$, i.e. the number of
non-zero entries in $A$. Further, a reduction size of
$t \in O(d^2)$ is already enough to obtain a subspace embedding.
This is independent of the number of data points $n$ and
can be considered a huge advantage.

Surprisingly, the suggested
procedure can easily be described in only two simple steps:
In the first step, an all zero matrix
$\tilde{A} \in \mathbb{R}^{d^2 \times d}$ is initialized, where
the result of the multiplication $SA$ will be stored.
Next, each row of $A$ is first multiplied by $+1$ or $-1$
with equal probability and then randomly added to one of the
rows of $\tilde{A}$, also with equal probability.
It turns out, that this simple procedure already yields the
result of $\tilde{A} = SA$ for an embedding matrix $S$ which
represents the procedure, and that $\tilde{A}$ can be
computed not only in time $O(\operatorname{nnz}(A))$, but also
in a single row by row pass over the matrix $A$.

The first step of the fast leverage score approximation procedure
introduced in \cite{leverage-scores-drineas} is to apply such
an embedding procedure to the matrix, for which the leverage scores
should be approximated, in our case $\sqrt{D_w}Z$, which will
yield a matrix $\tilde{Z} \in \mathbb{R}^{d^2 \times d}$.
This can be done in one row by row pass over the data, because
every row just has to be multiplied randomly with +1 or -1 and
then added to one of the rows in the result matrix $\tilde{Z}$.
In the next step, we perform a QR decomposition
$\tilde{Z} = \tilde{Q} \tilde{R}$, which now takes
$O(d^4)$ time. The resulting
matrix $\tilde{R}$ can then be used to approximate
the leverage scores in a second pass over the data by computing
$\tilde{\ell_i} = \lVert w_i z_i \tilde{R}^{-1} \rVert_2^2$, where $z_i$
is the $i$-th row-vector of $Z$.

The computation of $\tilde{\ell_i}$ takes time $O(d^2)$, but
the authors of \cite{leverage-scores-drineas} suggest another
procedure to speed up this time to $O(d \log(n))$, which can be
a useful step if $d > \log(n)$.
Their idea is to apply a so-called Johnson-Lindenstrauss
transformation \cite{johnson-lindenstrauss} to $R^{-1}$
in order to reduce its size to only $d \times m$ elements,
where $m \in O(\log(n))$. What this means is that the matrix
$R^{-1} \in \mathbb{R}^{d \times d}$ is multiplied by a random matrix
$G \in \mathbb{R}^{d \times m}$, where each entry of $G$
follows a normal distribution with mean zero and variance
$\frac{1}{m}$, i.e. $G_{ij} \sim \mathcal{N}(0, \frac{1}{m})$.
The resulting product $R^{-1}G$ can be computed once in the beginning,
and after that every single row by row pass takes only
$O(d \log(n))$ time to compute the approximated leverage
score $\tilde{\ell}_i = \lVert \sqrt{w_i} z_i (R^{-1} G) \rVert_2^2$.
It was shown in \cite{leverage-scores-drineas}, that the
approximations $\tilde{\ell}_i$ of the leverage scores satisfy that
\begin{equation*}
    (1 - \epsilon) \ell_i \leq \tilde{\ell}_i \leq (1 + \epsilon) \ell_i
\end{equation*}
for $\epsilon > 0$, where $\ell_i$ is the true leverage score.
We thus have obtained a constant factor
approximation of the true leverage scores that can be computed
efficiently in only two passes over the data and we can argue,
that the constant factor approximation of the leverage scores
doesn't affect the asymptotic analysis of the sensitivity
framework (see theorem~\ref{theorem:sensitivity-framework}).
Thus, we can replace the true leverage scores with the approximated
leverage scores in our sampling distribution without having any
impact on the coreset size.

\subsubsection{Putting it all together}

We can combine the idea of reservoir sampling with the fast approximation
method of the statistical leverage scores to improve
the running time of the na\"ive algorithm
in section~\ref{sec:naive-algorihtm}, so that it now only takes two
passes over the data. The resulting new algorithm is
given in Algorithm~\ref{algo:two-pass}.

In a first pass, the subspace embedding of \cite{woodruff-2017}
is applied to the scaled model matrix $Z$, which takes
$O(\operatorname{nnz}(Z))$ computation time. The subsequent
QR decomposition of the reduced $d^2 \times d$ matrix then runs
in time $O(d^4)$ and the computation of $\tilde{R}^{-1}$
as well as the Johnson Lindenstrauss transformation take
$O(d^3)$ time, because the computation is dominated by the
matrix inversion.

In the second pass, every row is fed into each of the
$k$ independent reservoir samplers, which each have a reservoir
of size one. This ensures, that the samples are drawn
with replacement. If the Johnson Lindenstrauss transformation
was applied, the second pass runs in time
$O(\operatorname{nnz}(Z) \log(n))$.
The resulting coreset is then simply the content of the
$k$ reservoirs of the independent reservoir samplers
and the total running time of the algorithm is
$O(\operatorname{nnz}(Z) \log(n) + \operatorname{poly}(d))$,
which is dominated by the second pass over the data.

\begin{algorithm}
    \DontPrintSemicolon
    \KwIn{Dataset $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$ with
    weight vector $w \in \mathbb{R}_{>0}^n$,
    $\mathcal{W} = \sum_{i=1}^n w_i$,
    size parameter $k \in \mathbb{N}$}
    \KwOut{A subset $\mathcal{C} \subseteq \mathcal{D}$
        of size $|\mathcal{C}| = k$ with weight vector
        $u \in \mathbb{R}_{>0}^k$}
    Initialize $\tilde{Z} = 0 \in \mathbb{R}^{d^2 \times d}$ \;
    \For(\tcp*[f]{first pass}){$i = 1,...,n$}{
        $j = $ random sample from $\{1, ..., d^2\}$ with equal probability \;
        $l = $ random sample from $\{+1, -1\}$ with equal probability \;
        $\tilde{Z}_j = \tilde{Z}_j + l \cdot \sqrt{w_i} x_i$
        \tcp*[r]{update the $j$'th row of $\tilde{Z}$}
    }
    Compute the QR decomposition of $\tilde{Z} = \tilde{Q}\tilde{R}$ \;
    Initialize $G = I \in \mathbb{R}^{d \times d}$ \;
    \If{$\lceil\log(n)\rceil < d$} {
        $G = 0 \in \mathbb{R}^{d \times \lceil\log(n)\rceil}$ \;
        Draw $G_{ij} \sim \mathcal{N}(0, \frac{1}{\lceil\log{n}\rceil})$
        \tcp*[r]{draw Johnson-Lindenstrauss matrix}
    }
    Compute $M = \tilde{R}^{-1} G$ \;
    Initialize $u = 0 \in \mathbb{R}^k$\tcp*[r]{empty weight vector}
    Initialize $k$ independent weighted size-1-reservoir samplers $S_1, ..., S_k$ \;
    \For(\tcp*[f]{second pass}){$i = 1, ..., n$}{
        $\tilde{\ell}_i = \lVert \sqrt{w_i} x_i M\rVert_2^2$
        \tcp*[r]{approximate leverage scores}
        $a_i = \tilde{\ell}_i + \frac{w_i}{\mathcal{W}}$
        \tcp*[r]{sensitiviy bound}
        $s_i = w_i 2^{\lceil\log(\frac{a_i}{w_i})\rceil}$
        \tcp*[r]{rounding to control VC dimension}
        \For{$j = 1, ..., k$} {
            Feed $(x_i, y_i)$ with sampling weight $s_i$ to $S_j$ \;
            \If{$S_j$ \textup{samples} $(x_i, y_i)$}{
                $u_j = \frac{w_j}{s_i k}$\tcp*[r]{unnormalized weights}
            }
        }
    }
    $\mathcal{C} := $ elements from the $k$ reservoirs \;
    $u = u \cdot \sum_{i=1}^n s_i$\tcp*[r]{normalize weights}
    \KwRet{$\mathcal{C}$, $u$}
    \caption{A fast two-pass algorithm for coreset construction\label{algo:two-pass}}
\end{algorithm}

\subsection{A One-Pass Online Algorithm}

The two-pass algorithm is a fast and practical procedure to
construct coresets in the context of probit regression in most
situations. But sometimes, situations arise where two passes
over the dataset are just not feasible.
What if the data arrives in real time, with many thousand
samples per second, and we simply don't
have enough storage to keep all the records?
In such situations, we need to make our sampling decisions
immediately: Do we include a new sample in the coreset, or
do we discard it forever?
For these situations, we need to come up with new algorithmic
ideas, because the two-pass sampling of our previous algorithm
is simply not up to the task.

The reason, why Algorithm~\ref{algo:two-pass} needs two passes,
is that it first computes a fast approximation of the leverage
scores in the first pass, and then obtains a sample in the
second pass.
If we want to do both, approximating the leverage scores as
well as obtaining the sample, we have to find a way to
approximate the leverage scores in an online manner,
i.e. when the data arrives row by row and we don't have
any knowledge of the data that is about to come in the form
of future samples.
In this work, we follow a train of thought by the
authors of \cite{online-row-sampling}, who investigated
how the statistical leverage scores can be approximated
in such an online scenario, where the data arrives row by row
and a sampling decision has to be made immediately.

The first and most important observation by the authors of
\cite{online-row-sampling} is that we can easily obtain
overestimates of the leverage scores of a matrix, if we
remove some of its rows.
To be more specific, consider the matrix
$A \in \mathbb{R}^{n \times d}$ and remember that the
$i$'th leverage score is given by $\ell_i = a_i^T (A^TA)^{-1} a_i$,
where the vector $a_i \in \mathbb{R}^d$ represents the
$i$'th row of $A$.
Now, imagine that the matrix $A_j$ only contains the first
$j$ rows of $A$, and that we use $A_j$ to compute the
approximation $\hat{\ell}_i = a_i^T (A_j^TA_j)^{-1} a_i$.
In \cite{online-row-sampling} it was shown, that in this
case we always have that $\hat{\ell}_i \geq \ell_i$, i.e.
our approximation $\hat{\ell}_i$ that was obtained by only
considering the first $j$ rows of $A$, is an upper bound
for the true leverage score $\ell_i$.

At this point, we have to remind ourselves of the core theorem
of the sensitivity framework
(theorem~\ref{theorem:sensitivity-framework}), which is the basis
for all our coreset construction endeavors.
In this theorem, the most critical aspect is to find upper bounds
on the sensitivity scores in order to obtain a sampling
distribution that can yield a coreset, and we accomplished
this by showing, that the statistical leverage scores are
upper bounds on the sensitivities for probit regression.
Now, consider the overestimates $\hat{\ell}_i$.
Obviously, because $\hat{\ell}_i \geq \ell_i$, these overestimates
could also be used in our sampling distribution.
As long as the sum of these overestimates is small enough, the
resulting sample will also be a coreset.

We now have the basis for our first na\"ive online algorithm.
For a $d$-dimensional dataset
$\mathcal{D} = \{ (x_i, y_i) \}_{i=1}^n$ that arrives in an
online manner, we can at every step $i$ maintain a matrix
$M^{(i)} = M^{(i-1)} + w_i x_i x_i^T$,
$M^{(0)} = 0 \in \mathbb{R}^{d \times d}$ that represents the sum
of the outer products of the first $i$ weighted rows, much like the matrix
$A_j^TA_j$ above. Then, we can obtain overestimates of the statistical
leverage scores by computing $\hat{\ell}_i = w_i x_i^T (M^{(i)})^\dagger x_i$,
where the $\dagger$ symbol denotes the Moore-Penrose pseudoinverse.
After one pass, the resulting scores
$\hat{\ell}_1, ..., \hat{\ell}_n$ are all overestimates of the
true scores of the weighted matrix $\sqrt{D_w}Z$, that we
needed for our original sampling distribution.
The pseudocode for this na\"ive algorithm is given in
Algorithm~\ref{algo:online-naive}.

\begin{algorithm}
    \DontPrintSemicolon
    \KwIn{Dataset $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$
    with weight vector $w \in \mathbb{R}_{>0}^n$,
    $\mathcal{W} = \sum_{i=1}^n w_i$,
    size parameter $k \in \mathbb{N}$}
    \KwOut{A subset $\mathcal{C} \subseteq \mathcal{D}$
        of size $|\mathcal{C}| = k$ with weight vector
        $u \in \mathbb{R}_{>0}^k$}
    Initialize $M^{(0)} = 0 \in \mathbb{R}^{d \times d}$ \;
    Initialize $k$ independent weighted size-1-reservoir
    samplers $S_1, ..., S_k$ \;
    \For{$i = 1, ..., n$} {
        $x_i = \sqrt{w_i} x_i$\tcp*[r]{rows are weighted}
        $M^{(i)} = M^{(i-1)} + x_i x_i^T$\tcp*[r]{rank one update}
        $\hat{\ell}_i = \min\{x_i^T (M^{(i)})^\dagger x_i, 1\}$\tcp*[r]{approximate leverage scores}
        $a_i = \hat{\ell}_i + \frac{w_i}{\mathcal{W}}$
        \tcp*[r]{sensitivity bound}
        $s_i = w_i 2^{\lceil\log(\frac{a_i}{w_i})\rceil}$
        \tcp*[r]{rounding to control VC dimension}
        \For{$j = 1, ..., k$} {
            Feed $(x_i, y_i)$ with sampling weight $s_i$ to $S_j$ \;
            \If{$S_j$ \textup{samples} $(x_i, y_i)$}{
                $u_j = \frac{w_j}{s_i k}$
                \tcp*[r]{unnormalized weights}
            }
        }
    }
    $\mathcal{C} := $ elements from the $k$ reservoirs \;
    $u = u \cdot \sum_{i=1}^n s_i$\tcp*[r]{normalize weights}
    \KwRet{$\mathcal{C}$, $u$}
    \caption{Na\"ive online algorithm for coreset construction\label{algo:online-naive}}
\end{algorithm}

In order to prove, that the na\"ive online algorithm does in fact
construct small coresets, we need to show that the sum of the
overestimates $\hat{\ell}_1, ..., \hat{\ell}_n$ is small,
because this sum does directly impact the coreset size in the
sensitivity framework.
Luckily, this work has already been done by the authors of
\cite{tensor-factorization} in an effort to build on the work
by \cite{online-row-sampling}.

\begin{lemma}[\cite{tensor-factorization}]
    \label{lemma:online-scores-sum}
    Let $A \in \mathbb{R}^{n \times d}$ and let
    $\hat{\ell}_1, ..., \hat{\ell}_n$ be overestimates of the statistical
    leverage scores of $A$ that were obtained by computing
    $\hat{\ell}_i = a_i^T (A_i^TA_i)^\dagger a_i$,
    where $A_i$ is the matrix that only consists of the first
    $i$ rows of $A$ and the vector $a_i \in \mathbb{R}^d$ represents the
    the $i$'th row of $A$.
    Then, it holds that
    \begin{equation*}
        \sum_{i=1}^n \hat{\ell}_i \in O(d + d \log \lVert A \rVert - \min_{i \in [n]} \lVert a_i \rVert).
    \end{equation*}
\end{lemma}

We can directly use this result in order to show that the na\"ive
algorithm is correct and indeed yields a small coreset by
only passing once over the dataset.

\begin{theorem}
    Algorithm~\ref{algo:online-naive} returns a $(1 \pm \epsilon)$-coreset
    for probit regression, if the size parameter $k$ satisfies
    \begin{equation*}
        TODO
    \end{equation*}
\end{theorem}
\begin{proof}
    TODO
\end{proof}

\begin{algorithm}
    \DontPrintSemicolon
    \KwIn{Dataset $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$
    with weight vector $w \in \mathbb{R}_{>0}^n$,
    $\mathcal{W} = \sum_{i=1}^n w_i$,
    size parameter $k \in \mathbb{N}$}
    \KwOut{A subset $\mathcal{C} \subseteq \mathcal{D}$
        of size $|\mathcal{C}| = k$ with weight vector
        $u \in \mathbb{R}_{>0}^k$}
    Initialize $M = M_{inv} = Q = 0 \in \mathbb{R}^{d \times d}$ \;
    Initialize $k$ independent weighted size-1-reservoir
    samplers $S_1, ..., S_k$ \;
    \For{$i = 1, ..., n$} {
    $x_i = \sqrt{w_i} x_i$\tcp*[r]{rows are weighted}
    $M = M + x_i x_i^T$\tcp*[r]{rank one update}
    \If(\tcp*[f]{$x_i$ in column space of Q?}){$\lVert Q x_i \rVert_2 = \lVert x_i \rVert_2$}{
    $M_{inv} = M_{inv} - \frac{M_{inv}x_ix_i^TM_{inv}}{1 + x_i^T M_{inv} x_i}$
    \tcp*[r]{adapted Sherman-Morrison formula}
    }
    \Else{
        $M_{inv} = M^\dagger$\tcp*[r]{Moore-Penrose pseudoinverse}
        $QR = M$\tcp*[r]{QR decomposition of M}
    }
    $\ell_i = \min\{x_i^T M_{inv} x_i, 1\}$\tcp*[r]{approximate leverage scores}
    $a_i = \ell_i + \frac{w_i}{\mathcal{W}}$
    \tcp*[r]{sensitivity bound}
    $s_i = w_i 2^{\lceil\log(\frac{a_i}{w_i})\rceil}$
    \tcp*[r]{rounding to control VC dimension}
    \For{$j = 1, ..., k$} {
        Feed $(x_i, y_i)$ with sampling weight $s_i$ to $S_j$ \;
        \If{$S_j$ \textup{samples} $(x_i, y_i)$}{
            $u_j = \frac{w_j}{s_i k}$
            \tcp*[r]{unnormalized weights}
        }
    }
    }
    $\mathcal{C} := $ elements from the $k$ reservoirs \;
    $u = u \cdot \sum_{i=1}^n s_i$\tcp*[r]{normalize weights}
    \KwRet{$\mathcal{C}$, $u$}
    \caption{Online algorithm for coreset construction\label{algo:online}}
\end{algorithm}
